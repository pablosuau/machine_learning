{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook. I replicate and document the code that I wrote with another member of my team during the **#HackTheNorth** event that took place in **Manchester on the 23rd and 24th of November 2017** (see https://dwpdigital.blog.gov.uk/2017/11/09/hackthenorth-helping-manchester-work-and-grow/). Such event, organised by DWP digital, involved the design and creation of data-based solutions aimed at produce a big impact on people's life - but specially focused on tackling the local unemployment problem.\n",
    "\n",
    "My team consisted of members with different backgrounds: DWP data scientists, back and front-end developers, business analysts and work coaches, among others. Together we worked on different elements of a complete system aimed at improving the job search experience at different levels. The system consisted on the following elements:\n",
    "\n",
    "- A machine learning model to classify jobs into either technical or non-technical solely based on the job description.\n",
    "- An application with the following features: a) build a job seeker's profile in terms of digital skills, based on a casual aptitude test, b) match the user to a set of technical/non-technical jobs based on the results of the previous step, and c) facilitate job search by means of a Tinder-like interface.\n",
    "\n",
    "The idea of the application was to reduce the frustration produced during job search when facing many different job offers that may not match our skills. \n",
    "\n",
    "Obviously, this was a very limited prototype due to the hack event's deadline, but many other ideas where considered, like for instance identifying spam jobs (by applying the same machine learning process used to classify jobs into technical and non-technical) or leveraging other sources of data (e.g. transport data to help filter jobs based on commute time and transport availability.)\n",
    "\n",
    "During the event, Izzy and I worked together on collecting and analysing job descriptions data towards the creation of a **simple document classification model**. Collecting the data was the most time-consuming step, by far, mainly due to the fact that we had to download and label the job descriptions manually. But this was the least of evils, because building a scrapper wouldn't have left us with enough time to work on the classification model. In spite of this, and thanks to the scikit-learn module, we were able to get a simple model up and running and iteratively improve it to the point of reaching a quite high accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, our text corpus was created by manually downloading job descriptions and manually labelling them as technical or non-technical. Our definition of technical or non-technical was based on whether any **digital skill** was required in order to perform the advertised job. Fortunately, we could look for jobs on **Universal Jobmatch** (https://jobsearch.direct.gov.uk/) based on skills. Other job searching platforms like Monster or Indeed only allow to search based on job title. Therefore, thanks to Universal Jobmatch, all we had to do was to thing of skills that could be considered as digital or non-digital, search jobs based on those skills, and download the job description section of each job.\n",
    "\n",
    "It must be noted that job descriptions were downloaded and stored without altering or making any change. Some job descriptions include the job title name, or even contact details. As the validation results at the bottom of this notebook show, including the job title may produce some biases on the classifier, but we decided to minimise the preprocessing of this data. \n",
    "\n",
    "For the purpose of this notebook I downloaded 24 technical and 24 non-technical job desriptions. They were stored in the *data/technical* and *data/non_technical* folders. This folder structure was chosen so that we could use scikit-learn's *load_files* method to easily load the documents and automatically assign them a label (the folder names technical and non_technical are automatically used by load_files to label the data.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = load_files('data/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows an example of one technical and one non-technical job description along with an example on how the labels are assigned to documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Software Developer- C#, Visual Studio, .Net, MVC- Chorley\\n\\nAbout the role\\n\\nRapid growth, expansion into new areas and demand from our customers has led to the need to expand our in-house software development team. We are now seeking a Software Developer to join the team. You will be responsible for playing a critical role in the development of new and existing applications and integrations in an agile environment.\\n\\nAbout Capita- Parking Eye\\n\\nWe are Capita, the UK's leading provider of business process management and integrated professional support service solutions. Through bespoke, quality solutions, we've helped countless organisations unlock value and maximise their potential. With access to our range of unique and diverse opportunities, offering real career advancement and progression, we can unlock your potential too.\\n\\nParkingEye (part of Capita) is the market leading car park management company. ParkingEye not only provide full circle car park management services but also real-time statistical and management information to clients across both the public and private sector utilising the latest in Automatic Number Plate Recognition technology. Managing the largest private car park management databases in Europe, our solutions are in place throughout the UK. Our solution ranges from large multi-node SQL Server databases, load balanced web applications and web services with thousands of data collection devices, touch screen kiosks, mobile applications and embedded vehicle systems out in the field.\\n\\nWhat you will do\\n\\nDesigning, developing, implementing, and releasing high quality custom software solutions\\nInvestigating (and resolve if appropriate) technical issues and support the BAU process\\nDesigning solutions in line with ParkingEye's requirements and modelling into software designs.\\nCarrying out transformation of software designs to programme logic.\\nAnalysing existing solutions to highlight areas for improvement\\nHelp improve our code quality through writing unit tests, automation and performing code reviews\\nWillingness to take on additional responsibility to ensure team success and quality solutions\\nTeam player that understands how their own work is important to the team's success\\nAbility to multitask across different projects\\nLead on discrete projects, development and design\\nProvide specialist information advice to other members of staff, managers and directors.\\nYour Experience Will Include:\\n\\nC#, Asp.Net MVC 4 and 5\\nExperience in Visual Studio 2013 and 2015\\nExperience working with Microsoft SQL Server (MS SQL Server 2012 preferable)\\nExperience with T-SQL including but not limited to writing stored procedures, cursors, variables, table variables, temp tables, indexing and understanding query plans\\nOO Programming using Design Patterns, Inversion of Control\\nHTML, CSS\\nJavaScript, JQuery\\nEntity Framework\\nDesired Skills\\n\\nSignalR, Redis Cache\\nBootstrap, AutoMapper\\nTDD\\nNServiceBus\\nBamboo\\nPHP, MySQL\\nWhat's in it for you?\\n\\nAt Capita, training and development aren't optional extras: they're how we do our job. We will motivate you to perform at your peak, recognising your achievements and rewarding them appropriately. As well as a generous basic salary we also give you holiday, pension scheme and access to voluntary benefit options including; child care vouchers, share plan schemes, life assurance, holiday buy and many more designed to suit your own personal lifestyle. All of this, in a professional but fun environment.\\n\\nWhat we hope you will do next\\n\\nHelp us find out more about you by completing our short application process - click apply now.\\n\\nFollow Capita on twitter @capitacareers\\nFollow Capita on facebook @careersatcapita\\n\\nCapita Resourcing welcome applications from all suitably qualified people regardless of gender, race, disability, age or sexual orientation.\\n\\nCapita Resourcing is a trading name of Capita Resourcing Ltd. Services offered are those of an Employment Agency and Employment Business. Applicants will be required to register with us.\\n\\nIf you are successful with your application, you will need complete Capita's vetting and screening checks. This will include, but not be limited to, Reference Checks, a Criminality Check, Financial Probity Check, Sanctions Check and Media Check.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "technical\n"
     ]
    }
   ],
   "source": [
    "print(job_descriptions.target[0])\n",
    "print(job_descriptions.target_names[job_descriptions.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Redbooth Ltd require roofing joiners immediately \\n\\nRedbooth Ltd require roofing tilers immediately for works on new build housing projects around the stockton and north east areas. Must be time served experienced and proficient with slating and concrete roof tiling. \\n\\nPlease note this is an immediate start so please call us on 07510439507.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions.data[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "non_technical\n"
     ]
    }
   ],
   "source": [
    "print(job_descriptions.target[31])\n",
    "print(job_descriptions.target_names[job_descriptions.target[31]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we randomly split our training set into a training and a test set (70% and 30% of the data, respectively). We will train the model using the training set and produce a report of the classification results using the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(job_descriptions.data, job_descriptions.target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a document classifier, we have first to transform each document into a vector of numerical features. We first applied a **bag of words model**: each document is represented by means of a feature vector in which each feature is a word and the value of the feature is the number of occurrences of that word in the document. The *CountVectorizer* object below uses the whole set of words in the training set to represent each single document (after transforming to lower case.) The resulting data is very sparse, but scikit-learn uses a sparse representation to avoid memory issues. \n",
    "\n",
    "We used raw words instead of text tokens (word roots, for instance) to build the bag of words representation. *CountVectorizer* can be used to tokenize. This is left for future work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 2475)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the word count may not be advisable in the case in which there is a wide difference in the number of words between documents. We observed that technical job descriptions are on average longer than non-technical job descriptions. Therefore, we decided to use proportions, and more specifically the *TfidTransformer* class, which transforms the bag of words representation into a **tf-idf (term-frequency times inverse document-frequency) representation**. \n",
    "\n",
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a word in a given document is to scale down the impact of words that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 2475)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we transformed the data into a feature-based representation, we feed this data into a **Multinomial Naive Bayes classifier** in order to build a classification model. Naive Bayes is extensively used in text-related machine learning tasks due to the tradeoff between its high performance in this domain and its simplicity. The assumption that distribution of words on documents is multinomial is also very popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained we can predict the class (technical/non-technical) of the test set. Notice that we need to transform the raw data (the job descriptions) into the same tf-ifd representation that was applied to the training set. For this purpose we are invoking the transform method of the **same** *CountVectorizer* and *TfidfTransformer* objects which were fit above in order to build the representation of the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "y_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use the predicted labels to print a very handy report showing the precission, recall and f1-score for each class. There were no false positives in the case of non_technical documents, and there were no false negatives in the case of technical documents. Several non-technical documents were classifed as technical, but all the documents that were classified as non-technical were indeed non-technical. \n",
    "\n",
    "We will use the f1-score as an average measure of prediction accuracy. The obtained value (0.63) is not quite bad considering the simplicity of the approach. It is better than the baseline random prediction (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "non_technical       1.00      0.38      0.55         8\n",
      "    technical       0.58      1.00      0.74         7\n",
      "\n",
      "  avg / total       0.81      0.67      0.63        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, labels=None, target_names=job_descriptions.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipelines\n",
    "\n",
    "The training and prediction process can be simplified by using pipelines. Our basic model above requires three stages which have to be executed in order: counting the number of words, computing proportions and weighting according to the tf-idf criterion, and finally classification itself.\n",
    "\n",
    "We can represent this process by means of the pipeline below, in which we feed each stage with the output of the previous stage. The pipeline receives a list of stages as an input. Each stage consists of a name and an object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the pipeline definition to fit our basic model with one single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction also requires a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pip_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we obtain exactly the same results as in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "non_technical       1.00      0.38      0.55         8\n",
      "    technical       0.58      1.00      0.74         7\n",
      "\n",
      "  avg / total       0.81      0.67      0.63        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, labels=None, target_names=job_descriptions.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection: cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than randomly splitting the data intro training and test sets only once we can of course apply 10-fold cross validation to make a better use of our small dataset and get a better idea of what the actual accuracy rate may be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80000000000000004, 0.13540064007726602]\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_val_score(pip_clf, job_descriptions.data, job_descriptions.target, cv=10)\n",
    "print([np.mean(cv_scores), np.std(cv_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection: grid search\n",
    "\n",
    "Our document classification pipeline is built by concatenating three different stages. The behaviour of each stage can be modified by means of a set of hyperparameters, but so far we used the default values. In this section, we investigate the effect of modifying the value of some of these hyperparameters, as well as the impact of using different elements in our pipeline, like a different classification algorithm (Support Vector Machines) or removing the *TfidTransformer* step. \n",
    "\n",
    "Given a set ot possible hyperparameters value lists, the *GridSearchCV* class applies cross validation to each combination of hyperparameter values. We can then use this information to decide the values we should set our model's hyperparameters before training the model that we will deploy with our application. \n",
    "\n",
    "*GridSearchCV* also lets us use different objects in a given stage of the pipeline. Unfortunately, we cannot \"disable\" a stage. In the context of our document classification problem we realised that the tf-idf step actually decreased the prediction accuracy in the case of the basic model. Should we remove this stage or keep it? Since I cannot enable or disable stages of the pipeline, we had to define a wrapper fot the *TfidTransformer* class that would allow us to activate or deactivate it by means of a boolean parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfTransformerOptional(TfidfTransformer):\n",
    "    def __init__(self, activate=True):\n",
    "        super().__init__()\n",
    "        self.activate = activate\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        if self.activate:\n",
    "            return super().fit_transform(X, y)\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        if self.activate:\n",
    "            return super().transform(X, y)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a parameters dictionary (the *param_grid* variable below) to tell *GridSearchCV* about all the possibilities that we want to explore. In my example:\n",
    "\n",
    "- We defined a set of values for the *min_df* and *max_df* parameters for *CountVectorizer*. By using these parameters we can filter out words from the bag of words model that are not frecuent enough (*df_min*) or are too frequent (*df_max*)\n",
    "- We compared two classification algorithms: Multinomial Naive Bayes and Support Vector Machines. In the case of Support Vector Machines we tested different values of the regularisation parameter (*C*)\n",
    "- Finally, we gave the option to test the effect of including the *TfidTransformer* stage or not. \n",
    "\n",
    "Notice how we can set the number of jobs to be run in parallel during the grid search process by setting a value for the *GridSearch*'s *n_jobs* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...formerOptional(activate=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=3,\n",
       "       param_grid=[{'vect__min_df': array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5]), 'vect__max_df': array([ 0.6,  0.7,  0.8,  0.9,  1. ,  1.1]), 'tfidf__activate': [True, False], 'clf': [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)]}, {'vect__min_df': array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5]),...ty=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)], 'clf__C': [0.5, 1, 1.5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformerOptional()),\n",
    "                    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect__min_df': np.arange(0,0.6,0.1),\n",
    "        'vect__max_df': np.arange(0.6,1.1,0.1),\n",
    "        'tfidf__activate': [True, False],\n",
    "        'clf': [MultinomialNB()],\n",
    "    },\n",
    "    {\n",
    "        'vect__min_df': np.arange(0,0.6,0.1),\n",
    "        'vect__max_df': np.arange(0.6,1.1,0.1),\n",
    "        'tfidf__activate': [True, False],\n",
    "        'clf': [SVC()],\n",
    "        'clf__C': [0.5, 1, 1.5]\n",
    "    },\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pip_clf, cv=10, n_jobs=3, param_grid=param_grid)\n",
    "grid.fit(job_descriptions.data, job_descriptions.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, the best results were obtained when using the Multinomial Naive Bayes Classifier with *d_min* = 0.2 and *d_max* = 0.7 and without the tf-idf step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895833333333\n",
      "0.2\n",
      "0.7\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "--\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "mean_scores = list(grid.cv_results_['mean_test_score'])\n",
    "index_max_score = mean_scores.index(max(mean_scores))\n",
    "print(mean_scores[index_max_score])\n",
    "print(grid.cv_results_['param_vect__min_df'][index_max_score])\n",
    "print(grid.cv_results_['param_vect__max_df'][index_max_score])\n",
    "print(grid.cv_results_['param_clf'][index_max_score])\n",
    "print(grid.cv_results_['param_clf__C'][index_max_score])\n",
    "print(grid.cv_results_['param_tfidf__activate'][index_max_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the document classification model that we will deploy as part of the application: we use all the complete corpus of job descriptions that we downloaded from the Universal Jobmatch website and the pipeline definition that performed best in the process described above.\n",
    "\n",
    "We store the model into disk to use it in the future from different scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_clf = Pipeline([('vect', CountVectorizer(min_df = 0.2, max_df = 0.7)),\n",
    "                    ('tfidf', TfidfTransformerOptional(activate=False)),\n",
    "                    ('clf', MultinomialNB()),\n",
    "])\n",
    "pip_clf.fit(job_descriptions.data, job_descriptions.target) \n",
    "\n",
    "pickle.dump([pip_clf, job_descriptions.target_names], open('models/model.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our data with the validation model\n",
    "\n",
    "Let's test our document classification model! We downloaded a new corpus of job descriptions consisting of three technical and three non-tecnical job roles, according to our definition of what a technical job is (based on the requirement of digital skills.) To build this validation set we searched for jobs using completely different skills to those used to build the training dataset.\n",
    "\n",
    "The code below loads and classifies each document independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------ non_technical_01.txt ------------------------------------------\n",
      "Predicted class: non_technical\n",
      "------------------------------------------------------------------------------------\n",
      " \n",
      "WE HAVE A VACANCY FOR A CAR MECHANIC AT OUR CAR SALES IN LLANTWIT FARDRE.EXPERIENCE IS ESSENTIAL AS WILL BE DOING ALL ASPECTS OF MECHANICS INCLUDING HEAD GASKETS GEARBOX CHANGE AND CLUTCHES.DUTIES ALSO INCLUDE SERVICING CHANGING TYRES AND WORKING AS PART OF A TEAM TO REACH OUR TARGETS.A FULL DRIVING LICENSE IS ALSO VITAL AS THE JOB INVOLVES DRIVING BETWEEN OUR CAR SALES AND TO THE MOT STATION.THE POSITION IS FULL TIME AND PERMANENT HOURS AND WAGES TO BE DISCUSSED AT INTERVIEW PROCESS.PLEASE CALL REVOLUTION CARS FOR MORE DETAILS\n",
      "\n",
      "------------------------------------------ non_technical_02.txt ------------------------------------------\n",
      "Predicted class: technical\n",
      "------------------------------------------------------------------------------------\n",
      " \n",
      "Over our 25 years of trading, Axis has secured an impressive client base and delivered contracts of varying size and value.  Our impressive portfolio now exceeds 3000 units, growing daily and includes blue-chip companies, national institutions, hotels, local authorities and banks.  We offer complete design builds of installations to bespoke requirements, such as our Inclinator at the Millennium Bridge in London; as well as maintenance contracts, service contracts and complete refurbishments to existing elevators.  We boast a proven track record of reliability and longevity, conforming to the highest British Standards and Lift Regulations.\n",
      "  \n",
      " Axis is currently seeking an experienced Repair Lift Engineer to join our busy repair team. Ideally, you will be based near the M3 or M4 Corridor, so Slough, Bracknell, Maidenhead, Newbury, Swindon, Bath, Bristol, Farnborough, Basingstoke, Winchester and Eastleigh: you will be responsible for major repairs across these locations.  You will be qualified to at least NVQ 3 in Lift Engineering (or equivalent) and have in-depth knowledge of a variety of different makes of equipment.  Whilst you must be a team player, it is essential that as a repair engineer, you can also work on your own initiative. You will show us that you are organised, hardworking and loyal, promoting and working to the required company Health and Safety standards, at all times. \n",
      "  \n",
      " This is an excellent opportunity for an ambitious Lift Engineer who is looking to develop their career with a world renowned company.  We are interested in receiving applications from Lift Engineers, Escalator Engineers, Principle Engineers, Senior Engineers or Field Engineers who live in the M3/M4 Corridor and have a valid driving license\n",
      "  \n",
      " We are committed to the continuing development of every employee; therefore we encourage and strongly support career progression, with opportunities for promotion within our organisation and throughout the wider UTC global group.\n",
      "  \n",
      " Axis is an Equal Opportunity Employer.\n",
      " Please note that due to the high volume of responses that we receive for opportunities across the UK, we are unfortunately no longer able to respond to each application directly or to provide further individual feedback. We will however contact you should we select you to attend an interview. Should you not receive a reply from us within 4 weeks, please assume that your application on this occasion has proved unsuccessful.\n",
      "\n",
      "------------------------------------------ non_technical_03.txt ------------------------------------------\n",
      "Predicted class: non_technical\n",
      "------------------------------------------------------------------------------------\n",
      " \n",
      "Working 8am - 5pm Monday - Friday, \n",
      " Must be qualificatied and must hold the 17th edition addition certificate, \n",
      " Must have current registration BS761:2008\n",
      " Must be able to issue NICEIC certification this would be an advantage\n",
      " Experience in maintainance work to be able to do complete re-wiring on building sites, ability to install intruder and fire alarms would be an advantage.\n",
      " Must be able to run catsize cables, telephone,internet cables, air condition cables, fire alarm cables\n",
      " \n",
      " Must have on tools \n",
      " \n",
      " If interested contact Naz 07508170169, or send text message with info re experience\n",
      "\n",
      "------------------------------------------ technical_01.txt ------------------------------------------\n",
      "Predicted class: non_technical\n",
      "------------------------------------------------------------------------------------\n",
      " \n",
      "The role holder will be comfortable writing code in any language that can utilize using their skills to auto processes, rebuild old solutions using best practice or helping the rest of the team. They will need to adapt and enjoy the challenge of using anything possible to deliver data quality.\n",
      " \n",
      " Any of these languages is essential:\n",
      " \n",
      " Java\n",
      " .Net\n",
      " Python\n",
      " C\n",
      " C#\n",
      " C++\n",
      " JavaScript\n",
      " Description:\n",
      " \n",
      " Experience working in a project environment\n",
      " \n",
      " Previous work on databases essential.\n",
      " \n",
      " Developing new and improving existing controls as required\n",
      " \n",
      " Performing testing as required\n",
      "\n",
      "------------------------------------------ technical_02.txt ------------------------------------------\n",
      "Predicted class: technical\n",
      "------------------------------------------------------------------------------------\n",
      " \n",
      "Technical Data Analyst, Contract, Immediate Start, Weybridge \n",
      " \n",
      " Your new company\n",
      " Large Government Department currently undergoing major digital change and transformation programme. \n",
      " \n",
      " Your new role\n",
      " You will be involved in the planning, preparation and migration of data from legacy systems new solutions. \n",
      " You will be expected to support and take ownership of data analysis activities. \n",
      " You will support projects in engaging with stakeholders to negotiate timing and approach to the data migration. \n",
      " \n",
      " What you'll need to succeed\n",
      " You will have experience as a data analyst.\n",
      " You will have strong stakeholder management skills.\n",
      " You need extensive experience in describing complex data issues and their implications in business terms to non technical language. \n",
      " \n",
      " What you'll get in return\n",
      " Long Contract with competitive daily rate. \n",
      " You will be exposed to a major digital change and transformation project. \n",
      " \n",
      " What you need to do now\n",
      " If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now.\n",
      " If this job isn't quite right for you but you are looking for a new position, please contact us for a confidential discussion on your career.\n",
      " \n",
      " Hays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job you accept the T&C's, Privacy Policy and Disclaimers which can be found at hays.co.uk\n",
      "\n",
      "------------------------------------------ technical_03.txt ------------------------------------------\n",
      "Predicted class: technical\n",
      "------------------------------------------------------------------------------------\n",
      " \n",
      "MoJ are looking for talented people to join our digital team in London. You’ll take the lead in creating world-class digital services to transform the UK’s justice system.\n",
      " \n",
      " Working closely in teams, you’ll use the latest cloud and digital technologies to make rapid and meaningful improvements to public services. Keeping a relentless focus on user needs, you’ll make the justice system easier to use and help people do the things they need to do.\n",
      " \n",
      " Your work building better digital services has the potential to impact millions of people across the UK in a positive way.\n",
      " \n",
      " MoJ team has over 150 staff, with experts in web development, design and user research, and works with around 50 organisations, including the courts service, prison service and Government Digital Service. \n",
      " \n",
      " The key purpose of the role is to:\n",
      " \n",
      " Enable MoJ software development teams to develop quality services that modernise the way the justice system works for citizens, and empower citizens to easily engage with justice. Through architecting and building highly-available and resilient container, deployment, and monitoring platforms, you will be helping teams to rapidly prototype, deliver, and run, high-impact and high-value services for 21st century digital government.\n",
      " \n",
      " What you’ll be doing: \n",
      " \n",
      " • Supporting development teams with application configuration for deployment, monitoring and other automation\n",
      " \n",
      " • Architecting and building modern cloud infrastructure to host essential government services for citizens, ensuring highly available, resilient applications.\n",
      " \n",
      " • Operational management of Linux servers, delivering a complex web application stack\n",
      " \n",
      " • Building and configuring new server platforms and the automated tooling to do so\n",
      " \n",
      " • Working with developers to test, debug and troubleshoot issues and problems\n",
      " \n",
      " • Testing, debugging and troubleshooting of platform level problems\n",
      " \n",
      " • Supporting development teams with configuring applications for deployment\n",
      " \n",
      " • Work with third-party providers and government departments to support a variety of integrations\n",
      " \n",
      " • Working with product teams on a range of tools and services, improving products to meet user needs\n",
      " \n",
      " • Participating in sprint planning to work with developers and project teams to ensure projects are deployable and monitorable from the outside\n",
      " \n",
      " • As part of the team you will be expected to participate some of the 2nd line in-house support and Out-of-Hours support rotas and will be compensated for doing so. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Requirements\n",
      " OUTSIDE IR35.\n",
      " CV Submission Deadline: 29/11/17 @11:00.\n",
      " \n",
      " Skills & Qualifications\n",
      " \n",
      " Essential\n",
      " \n",
      " • Experience configuring and managing Linux servers for serving a dynamic website\n",
      " \n",
      " • Experience debugging a complex multi-server service\n",
      " \n",
      " • Scripting or basic programming skills\n",
      " \n",
      " • Familiarity with network protocols - TCP/IP, HTTP, SSL, etc.\n",
      " \n",
      " • Deploying and configuring machines in a cloud environment\n",
      " \n",
      " • Understanding continuous integration\n",
      " \n",
      " • Comfortable with configuration management tools: at least one of Salt, Chef, or Puppet\n",
      " \n",
      " • Previous experience deploying web services in Ruby or Python; or previous experience developing web applications in Ruby or Python\n",
      " \n",
      " • Experience working in an agile environment \n",
      " \n",
      " • Knowledge of the use of version control systems such as git or subversion\n",
      " \n",
      " Desirable\n",
      " \n",
      " • Understanding of techniques for management of encryption keys and certificates\n",
      " \n",
      " • Knowledge of the principles underlying public/private key encryption schemes.\n",
      " \n",
      " • Installation and management of open source monitoring tools\n",
      " \n",
      " • Experience with open source solutions and community\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = ['non_technical_01.txt', 'non_technical_02.txt', 'non_technical_03.txt',\n",
    "         'technical_01.txt', 'technical_02.txt', 'technical_03.txt']\n",
    "for fil in files:\n",
    "    with open('data/validation/' + fil, 'r') as f:\n",
    "        text = [' '.join(f.readlines())] # The input to the pipeline has to be an array of documents\n",
    "                                           # even if we are only planning to process a single document\n",
    "        pred = pip_clf.predict(text)\n",
    "        \n",
    "        h = '------------------------------------------'\n",
    "        print(h + ' ' + fil + ' ' + h)\n",
    "        print('Predicted class: ' + job_descriptions.target_names[pred[0]])\n",
    "        print(h + h)\n",
    "        print(' ')\n",
    "        print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of the validation results and conclusions\n",
    "\n",
    "The validation accuracy rate was 0.66. We missclassified one technical job and one non-technical job. \n",
    "\n",
    "- The non-technical lift engineer job (once again, we are assuming that a lift engineer does not require to use computers to do his/her job) was classified as technical. One possible explanation may be that the word \"engineer\" is frequent in technical jobs, but non-existent in any of the non-technical job descriptions. Therefore, our classification model is biased and will think that all engineers require digital skills in their jobs. We could fix this issue by adding non-technical job descriptions containing the word \"engineer\" or \"engineering\" to our training dataset. We already did this in the past when we detected that we added some NHS related jobs to the technical set: the solution was to add non-technical NHS-related jobs. \n",
    "- The missclassified technical job is more of a mystery. All the relevant words seem to be there: there are many programming language names or IT related technologies. May it be that these words were filtered out by the bag of words model becuase there are not that frequent? Another possibility is the length of that job description. We observed that, in average, technical job descriptions tend to be much longer and more detailed. Should we add shorter technical job descriptions to our training set?\n",
    "\n",
    "I am surprised that such a simple model seems to work that well. However, the results have to be taken with a pinch of salt due to the small size of the dataset. We have to consider random variation. Aside from increasing the size of the dataset, I can think of the following ways in which we could improve this work:\n",
    "\n",
    "- Data scrapping: we would still have to manually label the job descriptions, but we could at least automatically assign labels based on the skill names used to search for jobs. \n",
    "- Further model selection: we didn't test all the possibilities. For instance, we didn't test the impact of the alpha parameter in the case of the Naive Bayes classifier. And we only tested two classifiers. \n",
    "- Using tokens instead of raw words will help to correctly group together related words that should have the same interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
