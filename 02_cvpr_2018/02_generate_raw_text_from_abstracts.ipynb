{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to parse the PDF files and extract abstracts' text to plain text files.\n",
    "\n",
    "**NOTE**: running this notebook requires the installation of the pdfminer2 package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pdf_file = 'papers/Zuffi_Lions_and_Tigers_CVPR_2018_paper.pdf'\n",
    "pdf_file = 'papers/Yu_DoubleFusion_Real-Time_Capture_CVPR_2018_paper.pdf'\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    \n",
    "    return ' '.join(text.split('\\n')).replace('- ','')\n",
    "\n",
    "text = convert_pdf_to_txt(pdf_file)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\n",
    "    'papers/Chen_Robust_Video_content_cvpr_2018_paper.pdf', # Broken link\n",
    "    'papers/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.pdf', # Broken link\n",
    "    'papers/Larsson_Beyond_Grobner_Bases_CVPR_2018_paper.pdf', # Broken link\n",
    "    'papers/Liu_Exploring_Disentangled_Feature_CVPR_2018_paper.pdf' # The parser gets stuck\n",
    "    ]\n",
    "## Manually deleting some papers \n",
    "for d in to_delete:\n",
    "    if os.path.exists(d):\n",
    "        os.remove(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the raw text from the papers\n",
    "papers = sorted(glob.glob('papers/*.pdf')) # Sorting alphabetically makes\n",
    "                                           # debugging easier\n",
    "    \n",
    "for i in tqdm(range(len(papers))):\n",
    "    paper = papers[i]\n",
    "    output_file = os.path.join('data/', os.path.basename(paper)).replace('.pdf', '.txt')\n",
    "    if not os.path.exists(output_file):\n",
    "        text = convert_pdf_to_txt(paper)\n",
    "        try:\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(text)\n",
    "        except:\n",
    "            os.remove(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a dataframe with papers and lens, distribution of lens, analysis of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: it should be enough to get what's in between abstract and 1. Introduction, but sometimes\n",
    "# specially if there is a figure at the beginning, this may not work. \n",
    "# For each pdf extract what it is between abstract and introduction, and take a look at the distribution\n",
    "# of the length of abstracts to detect weird cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
