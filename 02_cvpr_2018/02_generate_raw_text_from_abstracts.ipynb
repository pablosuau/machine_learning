{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import io\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to parse the PDF files and extract their text to plain text files. I am also looking for odd results after parsing.\n",
    "\n",
    "**NOTE**: running this notebook requires the installation of the pdfminer2 package. \n",
    "\n",
    "The function below is based on PDFMiner's source coude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DoubleFusion: Real-time Capture of Human Performances with Inner Body  Shapes from a Single Depth Sensor  Tao Yu1,2, Zerong Zheng1, Kaiwen Guo1,3, Jianhui Zhao2, Qionghai Dai1,  Hao Li4, Gerard Pons-Moll5, Yebin Liu1,6  1Tsinghua University, Beijing, China  2Beihang University, Beijing, China  3Google Inc  4University of Southern California / USC Institute for Creative Technologies  5Max-Planck-Institute for Informatics, Saarland Informatics Campus  6Beijing National Research Center for Information Science and Technology (BNRist)  Abstract  We propose DoubleFusion, a new real-time system that combines volumetric dynamic reconstruction with datadriven template ﬁtting to simultaneously reconstruct detailed geometry, non-rigid motion and the inner human body shape from a single depth camera. One of the key contributions of this method is a double layer representation consisting of a complete parametric body shape inside, and a gradually fused outer surface layer. A pre-deﬁned node graph on the body surface parameterizes the nonrigid deformations near the body, and a free-form dynamically changing graph parameterizes the outer surface layer far from the body, which allows more general reconstruction. We further propose a joint motion tracking method based on the double layer representation to enable robust and fast motion tracking performance. Moreover, the inner body shape is optimized online and forced to ﬁt inside the outer surface layer. Overall, our method enables increasingly denoised, detailed and complete surface reconstructions, fast motion tracking performance and plausible inner body shape reconstruction in real-time. In particular, experiments show improved fast motion tracking and loop closure performance on more challenging scenarios.  1. Introduction  Human performance capture has been a challenging research topic in computer vision and computer graphics for decades. The goal is to reconstruct a temporally coherent representation of the dynamically deforming surface of human characters from videos. Although array based methods [21, 12, 5, 6, 41, 22, 27, 11, 16, 30] using multiple video or depth cameras are well studied and have achieved high quality results, the expensive camera-array setups and  Figure 1: Our system and the real-time reconstructed results.  controlled studios limit its application to a few technical experts. As depth cameras are increasingly popular in the consumer space (iPhoneX, Google Tango, etc.), the recent trend focuses on using more and more practical setups like a single depth camera [45, 13, 3]. In particular, by combining non-rigid surface tracking and volumetric depth integration, DynamicFusion like approaches [28, 15, 14, 34] allow real-time dynamic scene reconstruction using a single depth camera without the requirement of pre-scanned model templates. Such systems are low cost, easy to set up and promising for popularization; however, they are still restricted to controlled slow motions. The challenges are occlusions (single view), computational resources (real-time), loop closure and no pre-scanned template model.  BodyFusion [43] is the most recent work in the direction of single-view real-time dynamic reconstruction; It shows that regularizing non-rigid deformations with a skeleton is beneﬁcial to capture human performances. However, since the human joints are too sparse and it only uses the gradually fused surface for tracking, it fails during fast motions, especially when the surface is not yet complete. Moreover,  17287  \\x0cthe skeleton embedding performance relies heavily on the initialization step and is ﬁxed afterwards. Inaccurate skeleton embedding results in deteriorated tracking and deformation performance.  For human performance capture, besides the skeleton, body shape is also a very strong prior since it is loop closed and complete. To fully take advantage of both human shape and pose motion prior, we propose “DoubleFusion”: a single-view and real-time dynamic surface reconstruction system that simultaneously reconstructs general cloth geometry and inner body shape. In addition, we make each layer beneﬁt from each other. Based on the recent state-ofthe-art body model SMPL [24], we propose a double-layer surface representation consisting of an outer surface layer, and an inner body layer for reconstruction and depth registration. The observed outer surface is gradually fused and deformed while the shape and pose parameters of the inner body layer are also gradually optimized to ﬁt inside the outer surface. On one hand, the inner body layer is a complete model that allows to ﬁnd enough correspondences, especially when only partial surface is obtained; in addition, it places a constraint on where to fuse the geometry of the outer surface. On the other hand, the gradually fused outer surface provides increasingly more constraints to update the body shape and pose online. The two layers are solved sequentially in real-time.  Overall, our proposed DoubleFusion system offers the new ability to simultaneously reconstruct the inner body shape and pose as well as the outer surface geometry and motion in real-time. This is achieved by using only a single depth camera, and without pre-scanning efforts. Compared to systems that only reconstruct the outer surface like BodyFusion [43], we demonstrate substantially improved performance in handling fast motions. In contrast to systems specialized to capture the inner body [3], our approach can handle people wearing casual clothing, and it works in real-time. To enable the above advantages, we make the following technical contributions in this paper.  • We propose the double-layer representation (Section 3.1) for high quality and realtime human performance capture. We deﬁne the double node graph that contains an on-body node graph and a far-body node graph. The double node graph enables better leverage of the human shape and pose prior, while still maintaining the ability to handle surface deformations that are far from the inner body surface. The double-layer representation may also be used in other human performance capture setups like multiview systems.  • Joint motion tracking (Section 4). We introduce a method to jointly optimize for the pose of the inner body shape and the non-rigid deformation of the outer surface based on the double-layer representation. Feature correspondences on both the inner body shape and  fused outer layer enable fast motion tracking performance and robust geometry reconstruction.  • Volumetric shape-pose optimization of the inner layer (Section 5). We ﬁt the SMPL model parameters with the canonical model directly in the TSDF volume deﬁned by the outer surface without searching correspondences. The optimized body shape and pose (skeleton embedding) in the canonical frame is beneﬁcial for outer surface tracking.  2. Related Work  In this work, we focus on capturing the dynamic geometry of human performer with detailed surface and personal body shape identity using a single depth sensor. The related methods can roughly divided into static template based, model-based and free-form reconstruction methods.  Static template based dynamic reconstruction. For performance capture, some of the previous works leverage pre-scanned templates. Thus surface reconstruction is turned into a motion tracking and surface deformation problem. Vlasic et al. [39] and Gall et al. [12] adopted a template with embedded skeleton driven by multi-view silhouettes and temporal feature constraints. Liu et al. [23] extended the method to handle multiple interacting performers. Some approaches [37, 32] use a random forest to predict correspondences to a template, and use them to ﬁt the template to the depth data. Ye et al. [41] considered the case of multiple Kinects input. Ye et al. [42] adopted a similar skinned model to estimate shape and pose parameters using a single-view depth camera in real-time. For this kind of template, in order to achieve accurate tracking, skeleton embedding is usually done manually.  Besides templates with an embedded skeleton, some works adopted template based non-rigid surface deformation. Li et al. [17] utilized embedded deformation graph in Sumner et al. [35] to parameterize the pre-scanned template to produce locally as-rigid-as-possible deformation. [13] adopted an ℓ0 norm constraint to generGuo et al. ate articulate motions without explicitly embedded skeleton. Zollh¨ofer et al. [45] took advantage of massive parallelism of GPU to enable real-time performance of general non-rigid tracking.  For the aforementioned require scanning a template step before capturing people with different identities or even the same performer with various apparels.  Model-based dynamic reconstruction. In addition to pre-scanned templates, many general body models have been proposed in the last decades. SCAPE [2] is one of the widely used model, it factorizes deformations into pose and shape components. SMPL [24] is a recent body model that represents shape and pose dependent deformations in an efﬁcient linear formulation. Dyna [31] learned a lowdimensional subspace to represent soft-tissue deformations.  7288  \\x0cMany research works utilized these shape priors to enforce more general constraints to capture dynamic bodies. [9] adopted SCAPE to capture body motion Chen et al. using a single depth camera. Bogo et al. [3] extended SCAPE to capture detailed body shape with appearance. Bogo et al. [4] used SMPL to ﬁt predicted 2D joint locations to estimate human shape and pose. However, neither SCAPE nor SMPL can represent arbitrary geometry of the performer wearing various apparels. In Zhang et al. [44] they addressed this problem by estimating the inner shape and recovering surface details. Pons-Moll et al. [30] introduce ClothCap, which jointly estimates clothing geometry and body shape using separate meshes. In both [44] and [30], results are only shown for complete 4D scan sequences. Alldieck et al. [1] reconstruct detailed shape including clothing from a monocular RGB video but the approach is off-line.  Free-form dynamic reconstruction. Free-form capture does not assume any geometric prior. For general non-rigid scenes, motion and geometry are closely coupled. In order to fuse regions visible in the future into a complete geometry, the algorithm needs to estimate non-rigid motion accurately. On the other hand, one needs accurate geometry to estimate motion accurately. In the last decades, many methods have been proposed to address free-form capture: linear variational deformation [20], deformation graph [18], subspace deformation [40], articulate deformation [7, 8] and [29], 4D spatio-temporal surface [26] and [36], incompressible ﬂows [33], animation cartography [38], quasi-rigid motions [19] and directional ﬁeld [10].  Only in recent years, free-form capture methods with real-time performance have been proposed. DynamicFusion [28] proposed a hierarchical node graph structure and an approximate direct GPU solver to enable capturing nonrigid scenes in real-time. Guo et al. [14] proposed a realtime pipeline that utilized shading information of dynamic scenes to improve non-rigid registration, meanwhile accurate temporal correspondences are used to estimate surface appearance. Innmann et al. [15] used SIFT features to improve tracking and Slavcheva et al. [34] proposed a killing constraint for regularization. However, neither of methods demonstrated full body performance capture with natural motions. Fusion4D [11] setup a rig with 8 depth camera to capture dynamic scenes with challenging motions in realtime. BodyFusion [43], utilizes skeleton priors for human body reconstruction, but cannot handle challenging fast motions and cannot infer inner body shape.  3. Overview  Figure 2: (a) Initialization of the on-body node graph. (b)(c)(d) Evaluation of the double node graph. The ﬁgure shows the geometry results and live node graph of (b) traditional free-form sampled node graph (red), (c) on-body node graph (green) only and (d) double node graph (with far-body nodes in blue). Note that we render the inner surface of the geometry in gray in (c)(top).  layer are observable surface regions, such as clothing, visible body parts (e.g. face, hair), while the inner layer is a parametric human shape and skeleton model based on the skinned multi-person linear model (SMPL) [24]. Similar to previous work [28], the motion of the outer surface is parametrized by a set of nodes. Every node deforms according to a rigid transformation. The node graph interconnects the nodes and constrain them to deform similarly. Unlike [28] that uniformly samples nodes on the newly fused surface, we pre-deﬁne an on-body node graph on the SMPL model, which provides a semantic and real prior to constrain non-rigid human motions. For example, it will prevent erroneous connections between body parts (e.g., connecting the legs). We uniformly sample on-body nodes and use geodesic distances to construct the predeﬁned on-body node graph on the mean shape of SMPL model as shown in Fig. 2(a)(top). The on-body nodes are inherently bound to skeleton joints in the SMPL model. Outer surface regions that are close to the inner body are bound to the on-body node graph. Deformations of regions far from the body cannot be accurately represented with the on-body graph. Hence, we additionally sample far-body nodes with a radius of δ = 5cm on the newly fused far-body geometry. A vertex is labled as far-body when it is located further than 1.4 × δcm from its nearest on-body node, which helps to make sure the sampling scheme is robust against depth noise and tracking failures. The double node graph is shown in Fig. 2(d)(bottom).  3.1. Double(cid:173)layer Surface Representation  3.2. Inner Body Model: SMPL  The input to DoubleFusion is a depth stream captured from a single consumer-level depth sensor and the output is a double-layer surface of the performer. The outer  SMPL [24] is an efﬁcient linear body model with N = 6890 vertices. SMPL incorporates a skeleton with K = 24 joints. Each joint has 3 rotational Degrees of Freedom  7289  \\x0c(DoF). Including the global translation of the root joint, there are 3 × 24 + 3 = 75 pose parameters. Before posing, the body model ¯T deforms according to shape parameters β and pose parameters θ to accommodate for different identities and non-rigid pose dependent deformations. Mathematically, the body shape T (β, θ) is morphed according to  T (β, θ) = ¯T + Bs(β) + Bp(θ)  (1)  where Bs(β) and Bp(θ) are vectors of vertex offsets, representing shape blendshapes and pose blendshapes respectively. The posed body model M (β, θ) is formulated as  M (β, θ) = W (T (β, θ), J(β), θ, W)  (2)  where W (·) is a general blend skinning function that takes the modiﬁed body shape T (β, θ), pose parameters θ, joint locations J(β) and skinning weights W, and returns posed vertices. Since all parameters were learned from data, the model produces very realistic shapes in different poses. We use the open sourced SMPL model with 10 shape blendshapes. See [24] for more details.  3.3. Initialization  During capture, we assume a ﬁxed camera position and treat camera movement as global scene rigid motion. In the initialization step, we require the performer to start with a rough A-pose. For the ﬁrst frame, we initialize TSDF volume by projecting depth map into the volume. Then we use volumetric shape-pose optimization (see Sec. 5.2) to estimate initial shape parameters β0 and skeletal pose θ0. After that, we initialize the double node graph using the on-body node graph and initial pose and shape as shown in Fig. 2(a)(bottom). We extract a triangle mesh from the volume using Marching Cube algorithm [25] and sample additional far-body nodes. These nodes are used to parameterize non-rigid deformations far from inner body shape.  3.4. Main Pipeline  The main challenge to adopt SMPL in our pipeline is that initially the incomplete outer surface leads to difﬁcult model ﬁtting. Our solution is to continuously update the shape and pose in the canonical frame when more geometry is fused. Therefore, we propose a pipeline that executes joint motion tracking, geometric fusion and volumetric shape-pose optimization sequentially (Fig. 3). We brieﬂy introduce the main components of the pipeline below: Joint Motion tracking Given the current estimated parameters of body shape, we jointly optimize pose and the non-rigid deformations deﬁned by the double node graph (Sec. 4). For the on-body nodes, we constrain the non-rigid deformations of them to follow skeletal motions. The farbody nodes are also optimized in the process but are not constrained by the skeleton.  Geometric fusion Similar to previous work [28], we nonrigidly integrate depth observation of multiple frames in a reference volume (Sec. 5.1). We also explicitly detect collided voxels to avoid erroneously fused geometry [14]. Volumetric shape-pose optimization After geometric fusion, the surface in the canonical frame gets more complete. We directly optimize the body shape and pose by using the fused signed distance ﬁeld (Sec. 5.2) This step is very efﬁcient because it does not require ﬁnding correspondences.  4. Joint Motion Tracking  There are two parameterizations in our motion tracking component, skeletal motions and non-rigid node deformations. Similar to the previous work [43], we adopt a binding term that constrains both motions to be consistent. Different from [43], we only enforce the binding term on onbody nodes to penalize non-articulated motions on on-body nodes. In contrast, far-body nodes have independent nonrigid deformations which are regularized to move like other nodes in the same graph structure. Besides geometric regularization, we also follow previous work [4] to use a statistic pose prior to prevent unnatural poses. The energy of joint optimization is then  Emot = λdataEdata + λbindEbind + λregEreg + λpriEpri, (3) where Edata, Ebind, Ereg and Eprior are energies of data, binding, regularization and pose prior term respectively. Data Term The data term measures the ﬁtting between the reconstructed double layer surface and depth map:  Edata = X(vc,u)∈P  τ1(vc) ∗ ψ(˜nT  vc (˜vc − u))+  (4)  (τ2(vc) + τ3(vc)) ∗ ψ(ˆnT  vc (ˆvc − u)),  where P is the correspondence set; ψ(·) is the robust Geman-McClure penalty function; (vc, u) is a correspondence pair; u is a sampled point on the depth map and its closest point vc can be on either the body shape or fused surface. Correspondences on the body shape enable fast and robust tracking performance. τ1(vc), τ2(vc) and τ3(vc) are correspondence indicator functions: τ1(vc) equals to 1 only if vc is on the fused surface; τ2(vc) equals to 1 when vc is on the body shape; τ3(vc) equals to 1 when vc is on the fused surface and its 4 nearest nodes (knn-nodes) of vc are all on-body nodes. ˜vc and ˜nvc are the vertex position and normal warped by its knn-nodes using dual quaternion blending and deﬁned as  T(vc) = SE3( Xk∈N (vc)  ω(k, vc) dqk),  (5)  where dqj is the dual quaternion of jth node; SE3(·) maps a dual quaternion to SE(3) space; N (vc) represents a set of  7290  \\x0cFigure 3: Our system pipeline. We ﬁrst initialize our system using the ﬁrst depth frame (Sec. 3.3). Then for each frame, we sequentially perform the next 3 steps: joint motion tracking ( Sec. 4), geometric fusion (Sec. 5.1) and volumetric shape-pose optimization (Sec. 5.2).  node neighbors of vc; ω(k, vc) = exp(−kvc−xkk2 k)) is the inﬂuence weight of the kth node xk to vc; we set the inﬂuence radius rk = 0.075m for all nodes. ˆvc and ˆnvc are the vertex position and its normal skinned by skeleton motions using linear blend skinning (LBS) and deﬁned as  2/(2r2  G(vc) =Xi∈B Gi = Yk∈Ki  wi,vc Gi,  exp(θk ˆξk),  (6)  where B is index set of bones; Gi is the cascaded rigid transformation of ith bone; wi,vc is the skinning weight associated with kth bone and point vc; Ki is parent indices of ith bone in the backward kinematic chain; exp(θk ˆξk) is the exponential map of the twist associated with kth bone. Note that the skinning weights of vc is given by the weighted average of the skinning weights of its knn-nodes.  For each u on the depth map, we search for two types of correspondences on our double layer surface: vt on the body shape and vs on the fused surface. We choose the one that maximizes the following metric based on Euclidean distance and normal afﬁnity  c = argmax  i∈{t,s}  (cid:18)1 −  kvi − uk2  δmax (cid:19)2  + µ ˜nT  vi nu! ,  (7)  where we choose µ = 0.2; we set δmax = 0.1m as the maximum radius used to search correspondences. We adopt two strategies for correspondence searching. To ﬁnd correspondences between the depth map and the fused surface, we project the fused surface to 2D and then ﬁnd correspondences within a local search window. For correspondences between the depth map and the body shape, we ﬁrst ﬁnd the nearest on-body node and then search for the nearest vertex  around it. We eliminate the correspondences with distance bigger than δmax. These two methods are efﬁcient for realtime performance and avoid building complex space partitioning data structure on GPU. The binding term attaches on-body nodes to their nearest bones and helps to produce articulated deformations on the body. It is deﬁned as  Ebinding = Xi∈Ls  kT(xi)xi − ˆxik2 2,  (8)  where Ls is the index set of on-body nodes. ˆxi is the node position skinned by LBS as deﬁned in Eqn. 6. Regularization Term The graph regularization is deﬁned on all of the graph edges. This term is used to produce locally as-rigid-as-possible deformations. For on-body node graph, we decrease the effects of this regularization around joint regions by comparing the skinning weight vector of neighboring nodes as in [43]. This term is then deﬁned as  Ereg =Xi Xj∈N (i)  ρ(kWi − Wjk2  2) kTixj − Tj xjk2  2 (9)  where Ti and Tj are transformation associated with ith and jth nodes; Wi and Wj are skinning weight vectors of these two nodes respectively; ρ(·) is the Huber weight function in [43]. Around joint regions, if two neighbor nodes are on different body parts, the difference of the skinning weight vectors is large, and thus ρ(·) will decrease the effect of the regularization. This will help to produce articulated deformations of on-body node graph. For far-body node graph, we construct its regularization term similar to [28]. Pose Prior Term Similar to [4], we include a pose prior penalizing the unnatural poses. It is deﬁned as  Eprior = − log(cid:16)Xj  ωjN (θ; µj, δj)(cid:17).  (10)  7291  \\x0cshape and poses parameters consistent with the previous ones. Epri is the same as in Eqn. 3 to prevent unnatural poses. The novel volumetric data term is deﬁned as  Esdata(β, θ) = X¯v∈ ¯T  ψ(D(W (T (¯v; β, θ); J(β), θ))),  (12) where D(·) is a bilinear sampling function that takes a point in the canonical volume and returns interpolated TSDF. Note that D(·) returns valid distance values only when the knn-nodes of the given point are all on-body nodes; otherwise D(·) returns 0. This prevents the body shape from incorrectly ﬁtting exterior objects, e.g., the backpack a performer is wearing. v = T (¯v; β, θ) modiﬁes ¯v by shape blend shape and pose blend shape; W (v; J(β, θ), θ) deforms v using linear blend skinning. The temporal regularization is deﬁned as  Esreg(β, θ, β′, θ′) = γ1kβ − β′k2  2 + γ2kθ − θ′k2 2.  (13)  This term prevents the optimized shape and pose parameters (β, θ) from deviating the ones (β′, θ′) of the previous frame.  Note that T (¯v; β, θ) includes both the pose and shape parameters, which makes W (v; J(β, θ), θ) a non-linear function. We ﬁnd that generally the pose blend shape Bp(θ) in T (¯v; β, θ) contributes much less to the modiﬁed body shape compared with the shape blend shape. Therefore we ignore the pose blend shape in T (¯v; β, θ), and the resulting skinning formulation W (T (¯v; β); J(β, θ), θ) becomes a linear function of (β, θ). This will generate a better energy landscape for the sampling based energy (Eqn. 12) and make the convergence faster. Then we solve the resulting energy using the same GPU-based Gauss-Newton solver as in Sec. 4. At last, we update the body shape and pose that embedded into the canonical frame and recalculate the motion ﬁeld and the skeleton motions. After more surface observation is fused into the TSDF volume, the body shape and canonical body pose get more accurate. (Fig. 4(b)).  Figure 4: Illustration of volumetric shape-pose optimization. (a) skeleton embedding results before and after optimization. (b) shape-mesh overlap before and after optimization.  This is formulated as a Gaussian Mixture Model (GMM), where ωj , µj and δj is the mixture weight, the mean and the variance of jth Gaussian model.  We solve the optimization problem (Eqn. 3) using Iterative Closest Point (ICP) method. First we build a correspondence set P using the latest motion parameters; then we solve the non-linear least squares using Gauss-Newton method. We use a twist representation for both the bone and node transformations. Within each iteration of GaussNewton procedure, the transformations are approximated using one-order Taylor expansion around the latest values. Then we solve the resulting linear system using a custom designed highly efﬁcient preconditioned conjugate gradient (PCG) solver on GPU [14, 11].  5. Volumetric Fusion & Optimization  5.1. Geometric Fusion  Similar to the previous non-rigid fusion works [28, 15, 14], we integrate the depth information into a reference volume. First, the voxels in the reference volume are warped to live frame according to current non-rigid warp ﬁeld. Then, we calculate the PSDF value of each valid voxel and use it to update their TSDF values. We follow the work [14] to cope with collided voxels in live frame to prevent erroneous fusion results caused by collisions.  5.2. Volumetric Shape(cid:173)Pose Optimization  6. Results  After the non-rigid fusion, we have an updated surface in the canonical volume with more complete geometry. Since the initial shape and pose parameters (β0, θ0) may not ﬁt well with the new observation in the volume, as shown in Fig.4(a), we propose a novel algorithm that can efﬁciently optimize both of the shape parameters and initial embedding pose jointly in the canonical volume. The formulation of the energy is then  In this section, we ﬁrst report the performance and the main parameters of the system. Then we compare with previous state-of-the-art methods qualitatively and quantitatively. We also evaluate each of our main contributions. In Fig. 5, we demonstrate the results of our system. Note the various shapes, challenging motions and different types of cloth of the loop closed model that we can reconstructed.  6.1. Performance  Eshape = Esdata + Esreg + Epri,  (11)  where Esdata measures misalignment error in the reference volume; Esreg is a temporal constraint that makes the new  DoubleFusion runs in real-time (running at 32ms per frame). The entire pipeline is implemented on one NVIDIA TITAN X GPU. Executing 6 ICP iterations, the joint motion tracking takes 21 ms. The geometric fusion takes 6 ms  7292  \\x0cFigure 5: Example results reconstructed by our system.  and volumetric shape-pose optimization takes 3 ms. Prior to the joint motion tracking, we process the input depth frame using bilateral ﬁltering, boundary outlier and ﬂoor plane removal. After volumetric shape-pose optimization, a triangulated mesh is extracted, non-rigidly transformed into camera coordinates and rendered on the frame. These two parts run asynchronously with the main pipeline, and runtime overhead is negligible with less than 1 ms. For all of our experiments, we choose λdata = 1.0, λbind = 1.0, λreg = 5.0 and λpri = 0.01. For each vertex, we use its 4 nearest neighbors for warping; for each node, we use its 8 nearest neighbors to construct the node graph. The size of the voxel is set to 4 mm in each dimension.  6.2. Evaluation  Double Node Graph. We evaluate the proposed double node graph in Fig. 2. The standard node graph construction scheme [35] uniformly samples all the nodes on the fused outer surface. The lack of semantic information results in wrong connections (connection between two legs) and erroneous fusion results as shown in Fig. 2(b). Using the on-body node graph alone is limited to capturing relatively tight clothing (e.g. the incomplete geometry of the backpack in Fig. 2(c)) since it is out of the control area of on-body node graph. By using the proposed double node graph (Fig. 2(d)), we can get clean and complete results. Joint motion tracking. In Fig. 6, we evaluate different components of the joint motion tracking step qualitatively. We eliminate non-rigid registration in Fig. 6(b) and (c). In Fig. 6(b), we only use correspondences on the body shape by setting τ1(vc) ≡ 0, τ3(vc) ≡ 0 in Eqn. 4. It shows that without detailed surface and non-rigid registration, although an approximate pose can be tracked, the fused surface is noisy and erroneous; In Fig. 6(c), we use correspondences on both body shape and fused surface by setting τ1(vc) ≡ 0, the pose and fused surface get better but still contain artifacts. Only using all the energy terms we can get accurate pose and fusion results as shown in Fig. 6(d). We also evaluate the on-body correspondences separately in Fig. 7. Only using fused surface for tracking will easily get failed: will quickly fail when the left arm reappears with large motion  Figure 6: Evaluation of joint motion tracking. (a) reference color image. (b) results only using correspondences on body for skeleton tracking, without non-rigid registration; (c) searching correspondences on both body and fused surface for skeleton tracking, without non-rigid registration; (d) using full energy terms.  Figure 7: Evaluation of on-body correspondences. (a) reference color image (b) results only using fused surface for tracking. (c) results using both body and fused surface for tracking.  in the scene due to the lack of surface geometry as shown in Fig. 7(b). Using both surface and body shape for tracking will generate more plausible results as shown in Fig. 7(c). Volumetric shape-pose optimization. We evaluate volumetric shape-pose optimization both qualitatively and quantitatively. To evaluate non-rigid tracking accuracy, in Fig. 8, we use a public 4D sequence. We ﬁrst render a single view depth sequence and then perform reconstruction using our system with/without optimization. The per-frame tracking error is calculated by averaging the point to plane error from the fused surface to the ground truth. We get better non-rigid tracking accuracy by using the optimization as shown in Fig. 8(a), and (b-c) demonstrates the reconstructed shape-mesh overlap with and without optimization. In Fig. 9 and Fig. 10, we evaluate the accuracy of our reconstructed shape. We obtain ground truth undressed shape using laser scanner. Then we capture the same subject with clothing using DoubleFusion. As shown in Fig. 9, our reconstructed body shapes are plausible even though the subjects are dressed. Fig. 10 shows the average shape reconstruction error along the sequence.  7293  \\x0cFigure 8: Evaluation of volumetric shape-pose optimization using non-rigid tracking accuracy. (a) average tracking error per frame, (b) reconstructed shape-mesh overlap with optimization, (c) reconstructed shape-mesh overlap without optimization.  Figure 12: Comparison. (a) reference color image. (b)(c)(d), results of DynamicFusion[28], BodyFusion[43] and our method.  advantage of a human skeletal constraint for better tracking ability. Fig. 12 shows that our method achieves improved tracking and loop closure performance than other methods. Please see the supplementary video for more details.  Figure 9: Per-vertex error of the reconstructed body shapes.  7. Discussion  Figure 10: Evaluation of the body shape estimation accuracy of our online shape-pose optimization method.  Figure 11: Comparison of tracking accuracy on sequence ”szq”.  Method Maximum Error (m) Average Error (m)  BodyFusion[43]  0.0554 0.0277  Ours 0.0458 0.0221  Table 1: Average numerical errors on the entire sequence.  6.3. Comparison  We compare our  tracking accuracy with BodyFusion [43] using their public vicon dataset. DoubleFusion obtains smaller per-frame max error (Fig. 11), and smaller average error (Tab. 1), especially during fast motions.  We qualitatively compare our method with two real-time state-of-the-art methods [28, 43]. [28] uses general nonrigid registration method without any prior, while [43] takes  Limitations Our system tends to over-estimate body size when users wear thick clothing, and reconstruction of very wide cloth remains challenging. We cannot handle geometry separations of the outer surface, this could be addressed incorporating the key-volume update method in [11]. Our current system can not handle human-object interactions, which we plan to address in future work.  Conclusion In this paper, we have demonstrated the ﬁrst method for real-time reconstruction of both clothing and inner body shape from a single depth sensor. Based on the proposed double surface representation, our system achieved better non-rigid tracking and surface loop closure performance than state-of-the-art methods. Moreover, the real-time reconstructed inner body shapes are visually plausible. We believe the robustness and accuracy of our approach will enable many applications, especially in AR/VR, gaming, entertainment and even virtual try-on as we also reconstruct the underlying body shape. For the ﬁrst time, with DoubleFusion, users can easily digitize themselves.  Acknowledgements This work is supported by the National key foundation for exploring scientiﬁc instrument of China No.2013YQ140517; NKBRP of China No.2014CB744201; the National NSF of China grant No.61522111, No.61531014, No.61233005; Shenzhen Peacock Plan KQTD20140630115140843; Changjiang Scholars and Innovative Research Team in University, No.IRT 16R02; Google Faculty Research Award; the Okawa Foundation Research Grant; the U.S. Army Research Laboratory under contract W911NF-14D-0005.  7294  \\x0cReferences  [1] T. Alldieck, M. Magnor, C. Theobalt, and G. Pons-Moll. IEEE  Video-based reconstruction of 3d people models. CVPR, 2018. 3  [2] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, and J. Davis. Scape: Shape completion and animation of people. ACM Transactions on Graphics, 24(3):408–416, July 2005. 2  [3] F. Bogo, M. J. Black, M. Loper, and J. Romero. Detailed full-body reconstructions of moving people from monocular RGB-D sequences. In IEEE ICCV, 2015. 1, 2, 3  [4] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and M. J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In IEEE ECCV, Lecture Notes in Computer Science. Springer International Publishing, 2016. 3, 4, 5  [5] D. Bradley, T. Popa, A. Sheffer, W. Heidrich, and T. Boubekeur. Markerless garment capture. In ACM Transactions on Graphics, volume 27, page 99. ACM, 2008. 1  [6] T. Brox, B. Rosenhahn, J. Gall, and D. Cremers. Combined region and motion-based 3d tracking of rigid and articulated objects. IEEE Trans. Pattern Anal. Mach. Intell., 32(3):402– 415, 2010. 1  [7] W. Chang and M. Zwicker. Range scan registration using reduced deformable models. In CGF, volume 28, pages 447– 456. Wiley Online Library, 2009. 3  [8] W. Chang and M. Zwicker. Global registration of dynamic range scans for articulated model reconstruction. ACM Transactions on Graphics, 30(3):26, 2011. 3  [9] Y. Chen, Z.-Q. Cheng, C. Lai, R. R. Martin, and G. Dang. Realtime reconstruction of an animating human body from a single depth camera. IEEE Transactions on Visualization and Computer Graphics, 22(8):2000–2011, 2016. 3  [10] M. Dou, H. Fuchs, and J.-M. Frahm. Scanning and tracking In IEEE  dynamic objects with commodity depth cameras. ISMAR, 2013. 3  [11] M. Dou, S. Khamis, Y. Degtyarev, P. Davidson, S. R. Fanello, A. Kowdle, S. O. Escolano, C. Rhemann, D. Kim, J. Taylor, et al. Fusion4d: real-time performance capture of challenging scenes. ACM Transactions on Graphics, 35(4):114, 2016. 1, 3, 6, 8  [12] J. Gall, C. Stoll, E. De Aguiar, C. Theobalt, B. Rosenhahn, and H.-P. Seidel. Motion capture using joint skeleton tracking and surface estimation. In IEEE CVPR, 2009. 1, 2  [13] K. Guo, F. Xu, Y. Wang, Y. Liu, and Q. Dai. Robust non-rigid motion tracking and surface reconstruction using l0 regularization. In IEEE ICCV, 2015. 1, 2  [14] K. Guo, F. Xu, T. Yu, X. Liu, Q. Dai, and Y. Liu. Real-time geometry, albedo and motion reconstruction using a single rgbd camera. ACM Transactions on Graphics, 2017. 1, 3, 4, 6  [15] M. Innmann, M. Zollh¨ofer, M. Nießner, C. Theobalt, and M. Stamminger. Volumedeform: Real-time volumetric nonrigid reconstruction. In IEEE ECCV, 2016. 1, 3, 6  [16] V. Leroy, J.-S. Franco, and E. Boyer. Multi-view dynamic shape reﬁnement using local temporal integration. In IEEE ICCV, 2017. 1  [17] H. Li, B. Adams, L. J. Guibas, and M. Pauly. Robust singleview geometry and motion reconstruction. In ACM Transactions on Graphics, volume 28, page 175. ACM, 2009. 2  [18] H. Li, R. W. Sumner, and M. Pauly. Global correspondence optimization for non-rigid registration of depth scans. In CGF, volume 27, pages 1421–1430. Wiley Online Library, 2008. 3  [19] H. Li, E. Vouga, A. Gudym, L. Luo, J. T. Barron, and G. Gusev. 3d self-portraits. ACM Transactions on Graphics, 32(6):187, 2013. 3  [20] M. Liao, Q. Zhang, H. Wang, R. Yang, and M. Gong. Modeling deformable objects from a single depth camera. In IEEE ICCV, 2009. 3  [21] Y. Liu, Q. Dai, and W. Xu. A point-cloud-based multiview stereo algorithm for free-viewpoint video. IEEE Transactions on Visualization and Computer Graphics, 16(3):407– 418, May 2010. 1  [22] Y. Liu, J. Gall, C. Stoll, Q. Dai, H. Seidel, and C. Theobalt. Motion capture of multiple characters using multiview image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2720–2735, 2013. 1  [23] Y. Liu, C. Stoll, J. Gall, H.-P. Seidel, and C. Theobalt. Markerless motion capture of interacting characters using multiview image segmentation. In IEEE CVPR, 2011. 2  [24] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: A skinned multi-person linear model. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 34(6):248:1–248:16, Oct. 2015. 2, 3, 4  [25] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. In ACM SIGGRAPH, pages 163–169, New York, NY, USA, 1987. ACM. 4  [26] N. J. Mitra, S. Fl¨ory, M. Ovsjanikov, N. Gelfand, L. J. Guibas, and H. Pottmann. Dynamic geometry registration. In SGP, pages 173–182, 2007. 3  [27] A. Mustafa, H. Kim, J. Guillemaut, and A. Hilton. General dynamic scene reconstruction from multiple view video. In IEEE ICCV, 2015. 1  [28] R. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In IEEE CVPR, 2015. 1, 3, 4, 5, 6, 8  [29] Y. Pekelny and C. Gotsman. Articulated object reconstruction and markerless motion capture from depth video. In CGF, volume 27, pages 399–408. Wiley Online Library, 2008. 3  [30] G. Pons-Moll, S. Pujades, S. Hu, and M. Black. ClothCap: Seamless 4D clothing capture and retargeting. ACM Transactions on Graphics, (Proc. SIGGRAPH), 36(4), 2017. 1, 3  [31] G. Pons-Moll,  J. Romero, N. Mahmood, and M. J. Black. Dyna: A model of dynamic human shape in motion. ACM Transactions on Graphics, (Proc. SIGGRAPH), 34(4):120:1–120:14, Aug. 2015. 2  [32] G. Pons-Moll, J. Taylor, J. Shotton, A. Hertzmann, and A. Fitzgibbon. Metric regression forests for correspondence estimation. International Journal of Computer Vision, pages 1–13, 2015. 2  7295  \\x0c[33] A. Sharf, D. A. Alcantara, T. Lewiner, C. Greif, A. Sheffer, N. Amenta, and D. Cohen-Or. Space-time surface reconstruction using incompressible ﬂow. ACM Transactions on Graphics, 27(5):110, 2008. 3  [34] M. Slavcheva, M. Baust, D. Cremers, and S. Ilic. KillingFusion: Non-rigid 3D Reconstruction without Correspondences. In IEEE CVPR, 2017. 1, 3  [35] R. W. Sumner, J. Schmid, and M. Pauly. Embedded deformation for shape manipulation. SIGGRAPH ’07, New York, NY, USA, 2007. ACM. 2, 7  [36] J. S¨ußmuth, M. Winter, and G. Greiner. Reconstructing animated meshes from time-varying point clouds. In CGF, volume 27, pages 1469–1476. Blackwell Publishing Ltd, 2008. 3  [37] J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon. The vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation. In IEEE CVPR, 2012. 2  [38] A. Tevs, A. Berner, M. Wand, I. Ihrke, M. Bokeloh, J. Kerber, and H.-P. Seidel. Animation cartography-intrinsic reconstruction of shape and motion. ACM Transactions on Graphics, 31(2):12, 2012. 3  [39] D. Vlasic, I. Baran, W. Matusik, and J. Popovi´c. Articulated mesh animation from multi-view silhouettes. In ACM Transactions on Graphics, volume 27, page 97. ACM, 2008. 2  [40] M. Wand, B. Adams, M. Ovsjanikov, A. Berner, M. Bokeloh, P. Jenke, L. Guibas, H.-P. Seidel, and A. Schilling. Efﬁcient reconstruction of nonrigid shape and motion from real-time 3d scanner data. ACM Transactions on Graphics, 28(2):15, 2009. 3  [41] G. Ye, Y. Liu, N. Hasler, X. Ji, Q. Dai, and C. Theobalt. Performance capture of interacting characters with handheld kinects. In IEEE ECCV. 2012. 1, 2  [42] M. Ye and R. Yang. Real-time simultaneous pose and shape estimation for articulated objects using a single depth camera. In IEEE CVPR, 2014. 2  [43] T. Yu, K. Guo, F. Xu, Y. Dong, Z. Su, J. Zhao, J. Li, Q. Dai, and Y. Liu. Bodyfusion: Real-time capture of human motion and surface geometry using a single depth camera. In IEEE ICCV, 2017. 1, 2, 3, 4, 5, 8  [44] C. Zhang, S. Pujades, M. Black, and G. Pons-Moll. Detailed, accurate, human shape estimation from clothed 3D scan sequences. In IEEE CVPR, 2017. 3  [45] M. Zollh¨ofer, M. Nießner, S. Izadi, C. Rehmann, C. Zach, M. Fisher, C. Wu, A. Fitzgibbon, C. Loop, C. Theobalt, et al. Real-time non-rigid reconstruction using an RGB-D camera. ACM Transactions on Graphics, 33(4):156, 2014. 1, 2  7296  \\x0c'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdf_file = 'papers/Zuffi_Lions_and_Tigers_CVPR_2018_paper.pdf'\n",
    "pdf_file = 'papers/Yu_DoubleFusion_Real-Time_Capture_CVPR_2018_paper.pdf'\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    \n",
    "    return ' '.join(text.split('\\n')).replace('- ','')\n",
    "\n",
    "text = convert_pdf_to_txt(pdf_file)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the raw text data from the PDF papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\n",
    "    'papers/Chen_Robust_Video_content_cvpr_2018_paper.pdf', # Broken link\n",
    "    'papers/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.pdf', # Broken link\n",
    "    'papers/Larsson_Beyond_Grobner_Bases_CVPR_2018_paper.pdf', # Broken link\n",
    "    'papers/Liu_Exploring_Disentangled_Feature_CVPR_2018_paper.pdf' # The parser gets stuck\n",
    "    ]\n",
    "## Manually deleting some papers \n",
    "for d in to_delete:\n",
    "    if os.path.exists(d):\n",
    "        os.remove(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [00:00<00:00, 100774.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract the raw text from the papers\n",
    "papers = sorted(glob.glob('papers/*.pdf')) # Sorting alphabetically makes\n",
    "                                           # debugging easier\n",
    "    \n",
    "for i in tqdm(range(len(papers))):\n",
    "    paper = papers[i]\n",
    "    output_file = os.path.join('data/', os.path.basename(paper)).replace('.pdf', '.txt')\n",
    "    if not os.path.exists(output_file):\n",
    "        text = convert_pdf_to_txt(paper)\n",
    "        try:\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(text)\n",
    "        except:\n",
    "            os.remove(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for files that may have not been correctly parsed by looking at the distribution of paper lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/Abdelhamed_A_High-Quality_Denoising_CVPR_...</td>\n",
       "      <td>43724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/Abu_Farha_When_Will_You_CVPR_2018_paper.txt</td>\n",
       "      <td>42135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/Acuna_Efficient_Interactive_Annotation_CV...</td>\n",
       "      <td>46621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/Agrawal_Dont_Just_Assume_CVPR_2018_paper.txt</td>\n",
       "      <td>48624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/Agudo_Image_Collection_Pop-Up_CVPR_2018_p...</td>\n",
       "      <td>37131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper    len\n",
       "0  data/Abdelhamed_A_High-Quality_Denoising_CVPR_...  43724\n",
       "1   data/Abu_Farha_When_Will_You_CVPR_2018_paper.txt  42135\n",
       "2  data/Acuna_Efficient_Interactive_Annotation_CV...  46621\n",
       "3  data/Agrawal_Dont_Just_Assume_CVPR_2018_paper.txt  48624\n",
       "4  data/Agudo_Image_Collection_Pop-Up_CVPR_2018_p...  37131"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = sorted(glob.glob('data/*.txt'))\n",
    "df = pd.DataFrame(columns=['paper', 'len'], index=range(len(papers)))\n",
    "\n",
    "i = 0\n",
    "for paper in papers:\n",
    "    with open(paper, 'r') as f:\n",
    "        text = f.readlines()\n",
    "    df.iloc[i, :] = [paper, len(text[0])]\n",
    "    i = i + 1\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1228b1588>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1228b1400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df['len'].values.tolist(), bins=100)\n",
    "ax.set_title('Distribution of papers\\' text lengths')\n",
    "plt.axvline(x=5000, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some odd occurences, like several papers have a length of zero or very close to zero, and an oulier on the righthand side of the plot. Which papers are these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>data/Zhou_Explicit_Loss-Error-Aware_Quantizati...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>data/Ravi_Show_Me_a_CVPR_2018_paper.txt</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>data/Wang_Modulated_Convolutional_Networks_CVP...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>data/Liu_Learning_Markov_Clustering_CVPR_2018_...</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>data/Saito_Maximum_Classifier_Discrepancy_CVPR...</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>data/Fang_Weakly_and_Semi_CVPR_2018_paper.txt</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>data/Mo_Uncalibrated_Photometric_Stereo_CVPR_2...</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>data/Hu_Sketch-a-Classifier_Sketch-Based_Photo...</td>\n",
       "      <td>2449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>data/Tung_Reward_Learning_From_CVPR_2018_paper...</td>\n",
       "      <td>88331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paper    len\n",
       "950  data/Zhou_Explicit_Loss-Error-Aware_Quantizati...     21\n",
       "572            data/Ravi_Show_Me_a_CVPR_2018_paper.txt     54\n",
       "753  data/Wang_Modulated_Convolutional_Networks_CVP...    110\n",
       "435  data/Liu_Learning_Markov_Clustering_CVPR_2018_...    218\n",
       "599  data/Saito_Maximum_Classifier_Discrepancy_CVPR...    385\n",
       "177      data/Fang_Weakly_and_Semi_CVPR_2018_paper.txt    768\n",
       "491  data/Mo_Uncalibrated_Photometric_Stereo_CVPR_2...    969\n",
       "262  data/Hu_Sketch-a-Classifier_Sketch-Based_Photo...   2449\n",
       "706  data/Tung_Reward_Learning_From_CVPR_2018_paper...  88331"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['len'] < 5000) | (df['len'] > 80000)].sort_values(by='len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original PDF papers seem to be all right, but the parsed txt files have different degrees of parsing errors. Given the small amount of papers which have these issues, we decided to discard them from our analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
