{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that reproduces the code in this article: https://pub.towardsai.net/nlp-zero-to-hero-with-python-2df6fcebff6e\n",
    "\n",
    "Required packages: Spacy and NLTK.\n",
    "\n",
    "To download the English language library for spacy:\n",
    "\n",
    "`python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the english language library\n",
    "load_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "'m\n",
      "going\n",
      "to\n",
      "meet\n",
      "M.S.\n",
      "Dhoni\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Word tokenisation\n",
    "string = 'I\\'m going to meet M.S. Dhoni.'\n",
    "words = load_en(string)\n",
    "for tokens in words:\n",
    "    print(tokens.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy----->happi\n",
      "happier----->happier\n",
      "happiest----->happiest\n",
      "happiness----->happi\n",
      "breathing----->breath\n",
      "fairly----->fairli\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemming - reducing words to their root\n",
    "pot_stem = PorterStemmer()\n",
    "words = ['happy', 'happier', 'happiest', 'happiness', 'breathing', 'fairly']\n",
    "for word in words:\n",
    "    print(word + '----->' + pot_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy----->happi\n",
      "happier----->happier\n",
      "happiest----->happiest\n",
      "happiness----->happi\n",
      "breathing----->breath\n",
      "fairly----->fair\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemming\n",
    "snow_stem = SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(word + '----->' + snow_stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "'m \t VERB \t 10382539506755952630 \t be\n",
      "happy \t ADJ \t 244022080605231780 \t happy\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "this \t DET \t 1995909169258310477 \t this\n",
      "happiest \t ADJ \t 244022080605231780 \t happy\n",
      "place \t NOUN \t 7512738811199700769 \t place\n",
      "with \t ADP \t 12510949447758279278 \t with\n",
      "all \t DET \t 13409319323822384369 \t all\n",
      "happiness \t NOUN \t 2779265004918961325 \t happiness\n",
      ". \t PUNCT \t 12646065887601541794 \t .\n",
      "It \t PRON \t 10239237003504588839 \t it\n",
      "feels \t VERB \t 5741770584995928333 \t feel\n",
      "how \t ADV \t 16331095434822636218 \t how\n",
      "happier \t ADJ \t 244022080605231780 \t happy\n",
      "we \t PRON \t 16064069575701507746 \t we\n",
      "are \t AUX \t 10382539506755952630 \t be\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation (same objective as stemming, but using morphological analysis instead of heuristics)\n",
    "string = load_en('I\\'m happy in this happiest place with all happiness. It feels how happier we are')\n",
    "for lem_word in string:\n",
    "    print(lem_word.text, '\\t', lem_word.pos_, '\\t', lem_word.lemma, '\\t', lem_word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whereafter', 'over', 'wherein', 'also', 'about', 'anyone', 'former', 'per', 'make', 'he', 'others', 'still', 'into', 'back', 'nevertheless', 'always', 'so', 'his', 'we', 'myself', 'except', 'by', 'get', 'ca', 'during', 'some', 'it', 'last', 'sometime', 'three', 'these', 'their', 'sixty', 'if', 'something', 'amount', 'off', 'too', 'else', 'anywhere', 'thru', '’d', 'how', 'thereafter', 'enough', 'twelve', 'there', 'becoming', 'show', 'at', 'already', 'to', 'much', '‘re', 'rather', 'him', 'what', 'herself', 'thereby', 'keep', 'call', 'himself', 'sometimes', 'seem', '‘d', 'or', 'would', 'toward', 'without', 'here', 'have', 'does', 'least', 'almost', '‘ll', 'across', 'upon', 'an', 'four', 'one', 'its', 'other', 'has', 'hereafter', 'be', 'hundred', 'such', 'behind', 'however', 'third', 'somehow', \"'m\", 'serious', 'around', 'seems', 'throughout', '‘s', 'fifty', 'your', 'latterly', 'out', 'mostly', 'for', 'fifteen', 'then', 'via', 'beyond', 'next', 'may', 'although', 'go', 'might', 'n‘t', 'beforehand', 'really', 'hers', 'every', 'down', 'someone', 'otherwise', 'nothing', 'towards', 'why', 'thereupon', 'are', 'whereupon', 'did', 'done', 'beside', 'whereas', '’m', 'alone', 'below', \"'ll\", 'two', 'yet', 'together', 'afterwards', 'besides', 'somewhere', 'herein', 'them', 'will', 'using', 'quite', 'more', 'seeming', 'often', 'bottom', 'of', 'through', 'only', 'everywhere', 'the', 'whence', 'mine', 'part', 'from', 'further', 'i', 'now', 'and', 'whole', 'against', 'us', 'never', 'do', 'wherever', 'less', 'any', '’s', 'itself', 'twenty', 'few', '‘m', 'nowhere', \"'ve\", 'elsewhere', 'full', 'first', 're', 'whereby', 'all', 'not', 'anyhow', 'than', 'is', 'most', 'was', 'had', 'hereupon', 'nobody', \"'s\", 'whether', 'either', 'while', 'noone', 'five', 'hence', 'put', 'this', 'whose', 'themselves', 'whom', 'several', 'in', 'move', \"'re\", 'been', 'say', 'become', 'yourselves', 'ours', 'whoever', 'same', 'both', 'because', 'our', 'formerly', 'my', 'therefore', '’re', 'became', 'whenever', 'everyone', 'should', \"'d\", 'anyway', 'thus', 'those', 'six', 'along', 'being', 'thence', 'her', 'neither', 'above', 'nor', 'meanwhile', 'namely', 'yours', 'eight', 'can', 'onto', 'they', 'various', 'latter', 'moreover', 'no', 'again', 'were', '’ll', 'on', 'when', 'am', '’ve', 'everything', 'indeed', 'unless', 'where', 'me', 'up', 'with', 'until', 'perhaps', 'nine', 'between', 'give', 'very', 'empty', 'side', 'ten', 'anything', 'even', 'which', 'seemed', 'see', 'though', 'used', 'under', 'top', 'each', 'well', 'before', 'many', 'who', 'own', 'made', 'once', 'as', 'another', 'please', 'regarding', 'forty', 'yourself', 'hereby', 'ourselves', 'whatever', 'that', 'therein', 'front', 'name', 'whither', 'she', '‘ve', 'could', \"n't\", 'eleven', 'must', 'due', 'since', 'becomes', 'you', 'within', 'doing', 'amongst', 'but', 'just', 'take', 'a', 'cannot', 'n’t', 'after', 'ever', 'none', 'among'}\n"
     ]
    }
   ],
   "source": [
    "# default english stop words in Spacy\n",
    "print(load_en.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of speech analysis\n",
    "string = load_en('This is an example of Python code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example\n",
      "NOUN\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "print(string[3])\n",
    "# Coarse tag\n",
    "print(string[3].pos_)\n",
    "# Fine tag\n",
    "print(string[3].tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{90: 2, 87: 1, 92: 2, 85: 1, 96: 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting by coarse tag\n",
    "string.count_by(spacy.attrs.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DET'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The keys in the dictionary above are tag ids, and the values are the counts for each tag\n",
    "# What tag is tag 90? (we have two of those)\n",
    "string.vocab[90].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England - GPE - Countries, cities, states\n",
      "the Department of Work and Pensions - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "# Named entity recognition\n",
    "string = load_en('Lewis, who lived in England by that time, worked for the Department of Work and Pensions')\n",
    "if string.ents:\n",
    "    for ner in string.ents:\n",
    "        print(ner.text + ' - '+ ner.label_ + ' - ' + \n",
    "               str(spacy.explain(ner.label_)))\n",
    "else:\n",
    "    print('No Entity Found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
