{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrievement of articles metadata (this example is for articles in the machine learning category) seems to be limited to 50000 results.\n",
    "\n",
    "To download in bulk we have to use https://arxiv.org/help/oa/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtomFeed(title=AtomTextConstruct(text_type=<AtomTextType.html: 'html'>, lang=None, value='ArXiv Query: search_query=cat:cs.LG&id_list=&start=10&max_results=10'), id_='http://arxiv.org/api/1K0+JJNvmhpwFqUDN6Sxt+rR4xI', updated=datetime.datetime(2020, 10, 10, 0, 0, tzinfo=tzoffset(None, -14400)), authors=[], contributors=[], links=[AtomLink(href='http://arxiv.org/api/query?search_query%3Dcat%3Acs.LG%26id_list%3D%26start%3D10%26max_results%3D10', rel='self', type_='application/atom+xml', hreflang=None, title=None, length=None)], categories=[], generator=None, subtitle=None, rights=None, icon=None, logo=None, entries=[AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Evaluation of the Performance of the Markov Blanket Bayesian Classifier\\n  Algorithm'), id_='http://arxiv.org/abs/cs/0211003v1', updated=datetime.datetime(2002, 11, 1, 18, 9, 56, tzinfo=tzutc()), authors=[AtomPerson(name='Michael G. Madden', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0211003v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0211003v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='I.2.6', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2002, 11, 1, 18, 9, 56, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for\\nconstruction of probabilistic classifiers. This paper presents an empirical\\ncomparison of the MBBC algorithm with three other Bayesian classifiers: Naive\\nBayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these\\nare implemented using the K2 framework of Cooper and Herskovits. The\\nclassifiers are compared in terms of their performance (using simple accuracy\\nmeasures and ROC curves) and speed, on a range of standard benchmark data sets.\\nIt is concluded that MBBC is competitive in terms of speed and accuracy with\\nthe other algorithms considered.'), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Approximating Incomplete Kernel Matrices by the em Algorithm'), id_='http://arxiv.org/abs/cs/0211007v1', updated=datetime.datetime(2002, 11, 7, 7, 21, 58, tzinfo=tzutc()), authors=[AtomPerson(name='Koji Tsuda', uri=None, email=None), AtomPerson(name='Shotaro Akaho', uri=None, email=None), AtomPerson(name='Kiyoshi Asai', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0211007v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0211007v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='I2.6; I5.2', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2002, 11, 7, 7, 21, 58, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='In biological data, it is often the case that observed data are available\\nonly for a subset of samples. When a kernel matrix is derived from such data,\\nwe have to leave the entries for unavailable samples as missing. In this paper,\\nwe make use of a parametric model of kernel matrices, and estimate missing\\nentries by fitting the model to existing entries. The parametric model is\\ncreated as a set of spectral variants of a complete kernel matrix derived from\\nanother information source. For model fitting, we adopt the em algorithm based\\non the information geometry of positive definite matrices. We will report\\npromising results on bacteria clustering experiments using two marker\\nsequences: 16S and gyrB.'), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Reliable and Efficient Inference of Bayesian Networks from Sparse Data\\n  by Statistical Learning Theory'), id_='http://arxiv.org/abs/cs/0309015v1', updated=datetime.datetime(2003, 9, 10, 13, 56, 41, tzinfo=tzutc()), authors=[AtomPerson(name='Dominik Janzing', uri=None, email=None), AtomPerson(name='Daniel Herrmann', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0309015v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0309015v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='K.3.2', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2003, 9, 10, 13, 56, 41, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='To learn (statistical) dependencies among random variables requires\\nexponentially large sample size in the number of observed random variables if\\nany arbitrary joint probability distribution can occur.\\n  We consider the case that sparse data strongly suggest that the probabilities\\ncan be described by a simple Bayesian network, i.e., by a graph with small\\nin-degree \\\\Delta. Then this simple law will also explain further data with high\\nconfidence. This is shown by calculating bounds on the VC dimension of the set\\nof those probability measures that correspond to simple graphs. This allows to\\nselect networks by structural risk minimization and gives reliability bounds on\\nthe error of the estimated joint measure without (in contrast to a previous\\npaper) any prior assumptions on the set of possible joint measures.\\n  The complexity for searching the optimal Bayesian networks of in-degree\\n\\\\Delta increases only polynomially in the number of random varibales for\\nconstant \\\\Delta and the optimal joint measure associated with a given graph can\\nbe found by convex optimization.'), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Toward Attribute Efficient Learning Algorithms'), id_='http://arxiv.org/abs/cs/0311042v1', updated=datetime.datetime(2003, 11, 27, 5, 34, 4, tzinfo=tzutc()), authors=[AtomPerson(name='Adam R. Klivans', uri=None, email=None), AtomPerson(name='Rocco A. Servedio', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0311042v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0311042v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='I.2.6', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2003, 11, 27, 5, 34, 4, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='We make progress on two important problems regarding attribute efficient\\nlearnability.\\n  First, we give an algorithm for learning decision lists of length $k$ over\\n$n$ variables using $2^{\\\\tilde{O}(k^{1/3})} \\\\log n$ examples and time\\n$n^{\\\\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision\\nlists that has both subexponential sample complexity and subexponential running\\ntime in the relevant parameters. Our approach establishes a relationship\\nbetween attribute efficient learning and polynomial threshold functions and is\\nbased on a new construction of low degree, low weight polynomial threshold\\nfunctions for decision lists. For a wide range of parameters our construction\\nmatches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives\\nan essentially optimal tradeoff between polynomial threshold function degree\\nand weight.\\n  Second, we give an algorithm for learning an unknown parity function on $k$\\nout of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$.\\nFor $k=o(\\\\log n)$ this yields a polynomial time algorithm with sample\\ncomplexity $o(n)$. This is the first polynomial time algorithm for learning\\nparity on a superconstant number of variables with sublinear sample complexity.'), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Improving spam filtering by combining Naive Bayes with simple k-nearest\\n  neighbor searches'), id_='http://arxiv.org/abs/cs/0312004v1', updated=datetime.datetime(2003, 11, 30, 20, 41, 18, tzinfo=tzutc()), authors=[AtomPerson(name='Daniel Etzold', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0312004v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0312004v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='I.2.6', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2003, 11, 30, 20, 41, 18, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Using naive Bayes for email classification has become very popular within the\\nlast few months. They are quite easy to implement and very efficient. In this\\npaper we want to present empirical results of email classification using a\\ncombination of naive Bayes and k-nearest neighbor searches. Using this\\ntechnique we show that the accuracy of a Bayes filter can be improved slightly\\nfor a high number of features and significantly for a small number of features.'), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='About Unitary Rating Score Constructing'), id_='http://arxiv.org/abs/cs/0401005v1', updated=datetime.datetime(2004, 1, 8, 7, 50, 51, tzinfo=tzutc()), authors=[AtomPerson(name='Kromer Victor', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0401005v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0401005v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='1.2.6', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2004, 1, 8, 7, 50, 51, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='It is offered to pool test points of different subjects and different aspects\\nof the same subject together in order to get the unitary rating score, by the\\nway of nonlinear transformation of indicator points in accordance with Zipf\\'s\\ndistribution. It is proposed to use the well-studied distribution of\\nIntellectuality Quotient IQ as the reference distribution for latent variable\\n\"progress in studies\".'), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Mining Heterogeneous Multivariate Time-Series for Learning Meaningful\\n  Patterns: Application to Home Health Telecare'), id_='http://arxiv.org/abs/cs/0412003v1', updated=datetime.datetime(2004, 12, 1, 16, 32, 49, tzinfo=tzutc()), authors=[AtomPerson(name='Florence Duchene', uri=None, email=None), AtomPerson(name='Catherine Garbay', uri=None, email=None), AtomPerson(name='Vincent Rialle', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0412003v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0412003v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='G.3', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2004, 12, 1, 16, 32, 49, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='For the last years, time-series mining has become a challenging issue for\\nresearchers. An important application lies in most monitoring purposes, which\\nrequire analyzing large sets of time-series for learning usual patterns. Any\\ndeviation from this learned profile is then considered as an unexpected\\nsituation. Moreover, complex applications may involve the temporal study of\\nseveral heterogeneous parameters. In that paper, we propose a method for mining\\nheterogeneous multivariate time-series for learning meaningful patterns. The\\nproposed approach allows for mixed time-series -- containing both pattern and\\nnon-pattern data -- such as for imprecise matches, outliers, stretching and\\nglobal translating of patterns instances in time. We present the early results\\nof our approach in the context of monitoring the health status of a person at\\nhome. The purpose is to build a behavioral profile of a person by analyzing the\\ntime variations of several quantitative or qualitative parameters recorded\\nthrough a provision of sensors installed in the home.'), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Stability Analysis for Regularized Least Squares Regression'), id_='http://arxiv.org/abs/cs/0502016v1', updated=datetime.datetime(2005, 2, 3, 19, 54, 2, tzinfo=tzutc()), authors=[AtomPerson(name='Cynthia Rudin', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0502016v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0502016v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2005, 2, 3, 19, 54, 2, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value=\"We discuss stability for a class of learning algorithms with respect to noisy\\nlabels. The algorithms we consider are for regression, and they involve the\\nminimization of regularized risk functionals, such as L(f) := 1/N sum_i\\n(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when\\ny_i is a noisy version of f*(x_i) for some function f* in H, the output of the\\nalgorithm converges to f* as the regularization term and noise simultaneously\\nvanish. We consider two flavors of this problem, one where a data set of N\\npoints remains fixed, and the other where N -> infinity. For the case where N\\n-> infinity, we give conditions for convergence to f_E (the function which is\\nthe expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we\\ndescribe the limiting 'non-noisy', 'non-regularized' function f*, and give\\nconditions for convergence. In the process, we develop a set of tools for\\ndealing with functionals such as L(f), which are applicable to many other\\nproblems in learning theory.\"), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Probabilistic and Team PFIN-type Learning: General Properties'), id_='http://arxiv.org/abs/cs/0504001v1', updated=datetime.datetime(2005, 3, 31, 23, 4, 28, tzinfo=tzutc()), authors=[AtomPerson(name='Andris Ambainis', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0504001v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0504001v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='F.1.1, I.2.6', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2005, 3, 31, 23, 4, 28, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='We consider the probability hierarchy for Popperian FINite learning and study\\nthe general properties of this hierarchy. We prove that the probability\\nhierarchy is decidable, i.e. there exists an algorithm that receives p_1 and\\np_2 and answers whether PFIN-type learning with the probability of success p_1\\nis equivalent to PFIN-type learning with the probability of success p_2.\\n  To prove our result, we analyze the topological structure of the probability\\nhierarchy. We prove that it is well-ordered in descending ordering and\\norder-equivalent to ordinal epsilon_0. This shows that the structure of the\\nhierarchy is very complicated.\\n  Using similar methods, we also prove that, for PFIN-type learning, team\\nlearning and probabilistic learning are of the same power.'), content=None, source=None), AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Non-asymptotic calibration and resolution'), id_='http://arxiv.org/abs/cs/0506004v4', updated=datetime.datetime(2006, 7, 1, 13, 46, 30, tzinfo=tzutc()), authors=[AtomPerson(name='Vladimir Vovk', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0506004v4', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0506004v4', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='I.2.6; I.5.1', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2005, 6, 1, 14, 3, 20, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='We analyze a new algorithm for probability forecasting of binary observations\\non the basis of the available data, without making any assumptions about the\\nway the observations are generated. The algorithm is shown to be well\\ncalibrated and to have good resolution for long enough sequences of\\nobservations and for a suitable choice of its parameter, a kernel on the\\nCartesian product of the forecast space $[0,1]$ and the data space. Our main\\nresults are non-asymptotic: we establish explicit inequalities, shown to be\\ntight, for the performance of the algorithm.'), content=None, source=None)])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import atoma\n",
    "url = 'http://export.arxiv.org/api/query?search_query=cat:cs.LG&start=10&max_results=10'\n",
    "response = requests.get(url)\n",
    "feed = atoma.parse_atom_bytes(response.content)\n",
    "print(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(feed.entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the Performance of the Markov Blanket Bayesian Classifier\n",
      "  Algorithm\n"
     ]
    }
   ],
   "source": [
    "print(feed.entries[0].title.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomPerson(name='Michael G. Madden', uri=None, email=None)]\n"
     ]
    }
   ],
   "source": [
    "print(feed.entries[0].authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002-11-01 18:09:56+00:00\n"
     ]
    }
   ],
   "source": [
    "print(feed.entries[0].published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for\n",
      "construction of probabilistic classifiers. This paper presents an empirical\n",
      "comparison of the MBBC algorithm with three other Bayesian classifiers: Naive\n",
      "Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these\n",
      "are implemented using the K2 framework of Cooper and Herskovits. The\n",
      "classifiers are compared in terms of their performance (using simple accuracy\n",
      "measures and ROC curves) and speed, on a range of standard benchmark data sets.\n",
      "It is concluded that MBBC is competitive in terms of speed and accuracy with\n",
      "the other algorithms considered.\n"
     ]
    }
   ],
   "source": [
    "print(feed.entries[0].summary.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtomEntry(title=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='Evaluation of the Performance of the Markov Blanket Bayesian Classifier\\n  Algorithm'), id_='http://arxiv.org/abs/cs/0211003v1', updated=datetime.datetime(2002, 11, 1, 18, 9, 56, tzinfo=tzutc()), authors=[AtomPerson(name='Michael G. Madden', uri=None, email=None)], contributors=[], links=[AtomLink(href='http://arxiv.org/abs/cs/0211003v1', rel='alternate', type_='text/html', hreflang=None, title=None, length=None), AtomLink(href='http://arxiv.org/pdf/cs/0211003v1', rel='related', type_='application/pdf', hreflang=None, title='pdf', length=None)], categories=[AtomCategory(term='cs.LG', scheme='http://arxiv.org/schemas/atom', label=None), AtomCategory(term='I.2.6', scheme='http://arxiv.org/schemas/atom', label=None)], published=datetime.datetime(2002, 11, 1, 18, 9, 56, tzinfo=tzutc()), rights=None, summary=AtomTextConstruct(text_type=<AtomTextType.text: 'text'>, lang=None, value='The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for\\nconstruction of probabilistic classifiers. This paper presents an empirical\\ncomparison of the MBBC algorithm with three other Bayesian classifiers: Naive\\nBayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these\\nare implemented using the K2 framework of Cooper and Herskovits. The\\nclassifiers are compared in terms of their performance (using simple accuracy\\nmeasures and ROC curves) and speed, on a range of standard benchmark data sets.\\nIt is concluded that MBBC is competitive in terms of speed and accuracy with\\nthe other algorithms considered.'), content=None, source=None)\n"
     ]
    }
   ],
   "source": [
    "print(feed.entries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sickle module to obtain arxiv's metadata through the OAI_PMH protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sickle import Sickle\n",
    "from requests.exceptions import HTTPError\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many machine learning articles in 2020 so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN: None\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C1001&verb=ListRecords\n",
      "128\n",
      "TOKEN: 4951178|1001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C2001&verb=ListRecords\n",
      "324\n",
      "TOKEN: 4951178|2001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C4001&verb=ListRecords\n",
      "912\n",
      "TOKEN: 4951178|4001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C6001&verb=ListRecords\n",
      "1601\n",
      "TOKEN: 4951178|6001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C8001&verb=ListRecords\n",
      "2450\n",
      "TOKEN: 4951178|8001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C9001&verb=ListRecords\n",
      "2802\n",
      "TOKEN: 4951178|9001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C10001&verb=ListRecords\n",
      "3181\n",
      "TOKEN: 4951178|10001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C11001&verb=ListRecords\n",
      "3536\n",
      "TOKEN: 4951178|11001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C12001&verb=ListRecords\n",
      "3987\n",
      "TOKEN: 4951178|12001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C13001&verb=ListRecords\n",
      "4432\n",
      "TOKEN: 4951178|13001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C15001&verb=ListRecords\n",
      "5244\n",
      "TOKEN: 4951178|15001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C16001&verb=ListRecords\n",
      "5635\n",
      "TOKEN: 4951178|16001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C18001&verb=ListRecords\n",
      "6276\n",
      "TOKEN: 4951178|18001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C20001&verb=ListRecords\n",
      "6959\n",
      "TOKEN: 4951178|20001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C21001&verb=ListRecords\n",
      "7313\n",
      "TOKEN: 4951178|21001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C26001&verb=ListRecords\n",
      "9422\n",
      "TOKEN: 4951178|26001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C27001&verb=ListRecords\n",
      "9821\n",
      "TOKEN: 4951178|27001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C28001&verb=ListRecords\n",
      "10166\n",
      "TOKEN: 4951178|28001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C29001&verb=ListRecords\n",
      "10481\n",
      "TOKEN: 4951178|29001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C31001&verb=ListRecords\n",
      "11138\n",
      "TOKEN: 4951178|31001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C32001&verb=ListRecords\n",
      "11476\n",
      "TOKEN: 4951178|32001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C33001&verb=ListRecords\n",
      "11793\n",
      "TOKEN: 4951178|33001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C34001&verb=ListRecords\n",
      "12164\n",
      "TOKEN: 4951178|34001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C36001&verb=ListRecords\n",
      "12794\n",
      "TOKEN: 4951178|36001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C37001&verb=ListRecords\n",
      "13132\n",
      "TOKEN: 4951178|37001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C39001&verb=ListRecords\n",
      "13784\n",
      "TOKEN: 4951178|39001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C40001&verb=ListRecords\n",
      "14126\n",
      "TOKEN: 4951178|40001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C42001&verb=ListRecords\n",
      "14792\n",
      "TOKEN: 4951178|42001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C43001&verb=ListRecords\n",
      "15113\n",
      "TOKEN: 4951178|43001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C44001&verb=ListRecords\n",
      "15468\n",
      "TOKEN: 4951178|44001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C46001&verb=ListRecords\n",
      "16446\n",
      "TOKEN: 4951178|46001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C47001&verb=ListRecords\n",
      "16979\n",
      "TOKEN: 4951178|47001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C51001&verb=ListRecords\n",
      "18834\n",
      "TOKEN: 4951178|51001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C57001&verb=ListRecords\n",
      "21015\n",
      "TOKEN: 4951178|57001\n",
      "25295\n",
      "TOKEN: 4951178|57001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C63001&verb=ListRecords\n",
      "27265\n",
      "TOKEN: 4951178|63001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C65001&verb=ListRecords\n",
      "27914\n",
      "TOKEN: 4951178|65001\n",
      "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C67001&verb=ListRecords\n",
      "28611\n",
      "TOKEN: 4951178|67001\n",
      "29575\n",
      "TOKEN: 4951178|67001\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C67001&verb=ListRecords",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-71b6cb453499>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                         'from': '2020-01-01'})\n\u001b[0;32m     12\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListRecords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'resumptionToken'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sickle\\app.py\u001b[0m in \u001b[0;36mListRecords\u001b[1;34m(self, ignore_deleted, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'verb'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ListRecords'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;31m# noinspection PyCallingNonCallable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_deleted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_deleted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mListIdentifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_deleted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sickle\\iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sickle, params, ignore_deleted)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'verb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVERBS_ELEMENTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'verb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOAIItemIterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msickle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_deleted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sickle\\iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sickle, params, ignore_deleted)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'verb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresumption_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sickle\\iterator.py\u001b[0m in \u001b[0;36m_next_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOAIItemIterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         self._items = self.oai_response.xml.iterfind(\n\u001b[0;32m    140\u001b[0m             './/' + self.sickle.oai_namespace + self.element)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sickle\\iterator.py\u001b[0m in \u001b[0;36m_next_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;34m'verb'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             }\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moai_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mharvest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         error = self.oai_response.xml.find(\n\u001b[0;32m     86\u001b[0m             './/' + self.sickle.oai_namespace + 'error')\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sickle\\app.py\u001b[0m in \u001b[0;36mharvest\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry_after\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mhttp_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mhttp_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mhttp_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 503 Server Error: Retry after specified interval for url: http://export.arxiv.org/oai2?resumptionToken=4951178%7C67001&verb=ListRecords"
     ]
    }
   ],
   "source": [
    "url = 'http://export.arxiv.org/oai2'\n",
    "count = 0\n",
    "token = None\n",
    "while 1:\n",
    "    print('TOKEN: ' + str(token))\n",
    "    sickle = Sickle(url)\n",
    "    if not token:\n",
    "        records = sickle.ListRecords(**{'metadataPrefix': 'arXiv',\n",
    "                                        'set': 'cs',\n",
    "                                        'ignore_deleted': True,\n",
    "                                        'from': '2020-01-01'})\n",
    "    else:\n",
    "        records = sickle.ListRecords(**{'resumptionToken': token})   \n",
    "    try:\n",
    "        for r in records:\n",
    "            if 'cs.LG' in r.metadata['categories'][0] or 'stat.ML' in r.metadata['categories'][0]:\n",
    "                #print(r.metadata)\n",
    "                count = count + 1\n",
    "    except HTTPError as e:\n",
    "        if e.response.status_code == 503:\n",
    "            m = re.search('resumptionToken=.+&', str(e).split(' ')[-1])\n",
    "            token = m.group(0)[:-1].replace('resumptionToken=', '').replace('%7C', '|')\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
